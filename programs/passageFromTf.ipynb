{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" src=\"images/dans-small.png\"/>\n",
    "<img align=\"right\" src=\"images/tf-small.png\"/>\n",
    "<img align=\"right\" src=\"images/etcbc.png\"/>\n",
    "\n",
    "\n",
    "#  Passage from Tf\n",
    "\n",
    "This creates passage databases for SHEBANQ.\n",
    "These are MYSQL tables, populated with data that SHEBANQ needs to show its pages.\n",
    "Most data comes from the BHSA repository, but we also need a few features from PHONO.\n",
    "\n",
    "We do not need material from PARALLELS and VALENCE, because they deliver their results for SHEBANQ\n",
    "in the form of sets of notes.\n",
    "\n",
    "## Discussion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os,sys,re,collections\n",
    "import utils\n",
    "from tf.fabric import Fabric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "See [operation](https://github.com/ETCBC/pipeline/blob/master/README.md#operation) \n",
    "for how to run this script in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if 'SCRIPT' not in locals():\n",
    "    SCRIPT = False\n",
    "    FORCE = True\n",
    "    VERSION= '4'\n",
    "\n",
    "def stop(good=False):\n",
    "    if SCRIPT: sys.exit(0 if good else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the context: source file and target directories\n",
    "\n",
    "The conversion is executed in an environment of directories, so that sources, temp files and\n",
    "results are in convenient places and do not have to be shifted around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "CORE_NAME = 'bhsa'\n",
    "PHONO_NAME = 'phono'\n",
    "\n",
    "repoBase = os.path.expanduser('~/github/etcbc')\n",
    "thisRepo = '{}/{}'.format(repoBase, CORE_NAME)\n",
    "phonoRepo = '{}/{}'.format(repoBase, PHONO_NAME)\n",
    "tfDir = 'tf/{}'.format(VERSION)\n",
    "\n",
    "thisTemp = '{}/_temp/{}'.format(thisRepo, VERSION)\n",
    "thisTempMysql = '{}/shebanq'.format(thisTemp)\n",
    "thisMysql = '{}/shebanq/{}'.format(thisRepo, VERSION)\n",
    "\n",
    "passageDb = 'shebanq_passage{}'.format(VERSION)\n",
    "\n",
    "mysqlZFile = '{}/{}.sql.gz'.format(thisMysql, passageDb)\n",
    "mysqlFile = '{}/{}.sql'.format(thisTempMysql, passageDb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "\n",
    "Check whether this conversion is needed in the first place.\n",
    "Only when run as a script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SCRIPT:\n",
    "    (good, work) = utils.mustRun(None, mysqlZFile, force=FORCE)\n",
    "    if not good: stop(good=False)\n",
    "    if not work: stop(good=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in (thisMysql, thisTempMysql):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect\n",
    "\n",
    "We collect the data from the TF repos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................\n",
      ".       0.00s Loading relevant features                                                      .\n",
      "..............................................................................................\n",
      "This is Text-Fabric 6.3.1\n",
      "Api reference : https://dans-labs.github.io/text-fabric/Api/General/\n",
      "Tutorial      : https://github.com/Dans-labs/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Example data  : https://github.com/Dans-labs/text-fabric-data\n",
      "\n",
      "104 features found and 0 ignored\n",
      "  0.00s loading features ...\n",
      "   |     0.01s B book                 from /Users/dirk/github/etcbc/bhsa/tf/4\n",
      "   |     0.00s B chapter              from /Users/dirk/github/etcbc/bhsa/tf/4\n",
      "   |     0.00s B verse                from /Users/dirk/github/etcbc/bhsa/tf/4\n",
      "   |     0.10s B g_cons               from /Users/dirk/github/etcbc/bhsa/tf/4\n",
      "   |     0.13s B g_cons_utf8          from /Users/dirk/github/etcbc/bhsa/tf/4\n",
      "   |     0.10s B g_lex                from /Users/dirk/github/etcbc/bhsa/tf/4\n",
      "   |     0.11s B g_word               from /Users/dirk/github/etcbc/bhsa/tf/4\n",
      "   |     0.15s B g_word_utf8          from /Users/dirk/github/etcbc/bhsa/tf/4\n",
      "   |     0.09s B lex                  from /Users/dirk/github/etcbc/bhsa/tf/4\n",
      "   |     0.14s B lex_utf8             from /Users/dirk/github/etcbc/bhsa/tf/4\n",
      "   |     0.07s B trailer_utf8         from /Users/dirk/github/etcbc/bhsa/tf/4\n",
      "   |     0.05s B g_qere_utf8          from /Users/dirk/github/etcbc/bhsa/tf/4\n",
      "   |     0.05s B qtrailer_utf8        from /Users/dirk/github/etcbc/bhsa/tf/4\n",
      "   |     0.08s B language             from /Users/dirk/github/etcbc/bhsa/tf/4\n",
      "   |     0.10s B sp                   from /Users/dirk/github/etcbc/bhsa/tf/4\n",
      "   |     0.09s B pdp                  from /Users/dirk/github/etcbc/bhsa/tf/4\n",
      "   |     0.09s B ls                   from /Users/dirk/github/etcbc/bhsa/tf/4\n",
      "   |     0.10s B g_entry              from /Users/dirk/github/etcbc/bhsa/tf/4\n",
      "   |     0.14s B g_entry_heb          from /Users/dirk/github/etcbc/bhsa/tf/4\n",
      "   |     0.09s B vt                   from /Users/dirk/github/etcbc/bhsa/tf/4\n",
      "   |     0.09s B vs                   from /Users/dirk/github/etcbc/bhsa/tf/4\n",
      "   |     0.08s B gn                   from /Users/dirk/github/etcbc/bhsa/tf/4\n",
      "   |     0.09s B nu                   from /Users/dirk/github/etcbc/bhsa/tf/4\n",
      "   |     0.09s B ps                   from /Users/dirk/github/etcbc/bhsa/tf/4\n",
      "   |     0.08s B st                   from /Users/dirk/github/etcbc/bhsa/tf/4\n",
      "   |     0.08s B nme                  from /Users/dirk/github/etcbc/bhsa/tf/4\n",
      "   |     0.09s B pfm                  from /Users/dirk/github/etcbc/bhsa/tf/4\n",
      "   |     0.09s B prs                  from /Users/dirk/github/etcbc/bhsa/tf/4\n",
      "   |     0.09s B uvf                  from /Users/dirk/github/etcbc/bhsa/tf/4\n",
      "   |     0.11s B vbe                  from /Users/dirk/github/etcbc/bhsa/tf/4\n",
      "   |     0.09s B vbs                  from /Users/dirk/github/etcbc/bhsa/tf/4\n",
      "   |     0.11s B gloss                from /Users/dirk/github/etcbc/bhsa/tf/4\n",
      "   |     0.06s B nametype             from /Users/dirk/github/etcbc/bhsa/tf/4\n",
      "   |     0.00s B root                 from /Users/dirk/github/etcbc/bhsa/tf/4\n",
      "   |     0.02s B pargr                from /Users/dirk/github/etcbc/bhsa/tf/4\n",
      "   |     0.18s B phono                from /Users/dirk/github/etcbc/bhsa/tf/4\n",
      "   |     0.06s B phono_sep            from /Users/dirk/github/etcbc/bhsa/tf/4\n",
      "   |     0.07s B function             from /Users/dirk/github/etcbc/bhsa/tf/4\n",
      "   |     0.16s B typ                  from /Users/dirk/github/etcbc/bhsa/tf/4\n",
      "   |     0.17s B rela                 from /Users/dirk/github/etcbc/bhsa/tf/4\n",
      "   |     0.02s B txt                  from /Users/dirk/github/etcbc/bhsa/tf/4\n",
      "   |     0.11s B det                  from /Users/dirk/github/etcbc/bhsa/tf/4\n",
      "   |     0.02s B code                 from /Users/dirk/github/etcbc/bhsa/tf/4\n",
      "   |     0.02s B tab                  from /Users/dirk/github/etcbc/bhsa/tf/4\n",
      "   |     0.17s B number               from /Users/dirk/github/etcbc/bhsa/tf/4\n",
      "   |     0.06s B freq_lex             from /Users/dirk/github/etcbc/bhsa/tf/4\n",
      "   |     0.07s B freq_occ             from /Users/dirk/github/etcbc/bhsa/tf/4\n",
      "   |     0.07s B rank_lex             from /Users/dirk/github/etcbc/bhsa/tf/4\n",
      "   |     0.06s B rank_occ             from /Users/dirk/github/etcbc/bhsa/tf/4\n",
      "  6.72s All features loaded/computed - for details use loadLog()\n"
     ]
    }
   ],
   "source": [
    "utils.caption(4, 'Loading relevant features')\n",
    "\n",
    "if VERSION in {'4', '4b'}:\n",
    "    QERE = 'g_qere_utf8'\n",
    "    QERE_TRAILER = 'qtrailer_utf8'\n",
    "    ENTRY = 'g_entry'\n",
    "    ENTRY_HEB = 'g_entry_heb' \n",
    "    PHONO_TRAILER = 'phono_sep'\n",
    "    LANGUAGE = 'language'\n",
    "else:\n",
    "    QERE = 'qere_utf8'\n",
    "    QERE_TRAILER= 'qere_trailer_utf8'\n",
    "    ENTRY = 'voc_lex'\n",
    "    ENTRY_HEB = 'voc_lex_utf8'\n",
    "    PHONO_TRAILER = 'phono_trailer'\n",
    "    LANGUAGE = 'languageISO'\n",
    "\n",
    "    \n",
    "TF = Fabric(locations=[thisRepo, phonoRepo], modules=[tfDir])\n",
    "api = TF.load(f'''\n",
    "        g_cons g_cons_utf8 g_word g_word_utf8 trailer_utf8\n",
    "        {QERE} {QERE_TRAILER}\n",
    "        {LANGUAGE} lex g_lex lex_utf8 sp pdp ls\n",
    "        {ENTRY} {ENTRY_HEB}\n",
    "        vt vs gn nu ps st\n",
    "        nme pfm prs uvf vbe vbs\n",
    "        gloss nametype root ls\n",
    "        pargr\n",
    "        phono {PHONO_TRAILER}\n",
    "        function typ rela txt det\n",
    "        code tab\n",
    "        number\n",
    "        freq_lex freq_occ\n",
    "        rank_lex rank_occ\n",
    "        book chapter verse\n",
    "''')\n",
    "api.makeAvailableIn(globals())\n",
    "\n",
    "hasLex = 'lex' in set(F.otype.all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data model\n",
    "\n",
    "The data model of the browsing database as as follows:\n",
    "\n",
    "There are tables ``book``, ``chapter``, ``verse``, ``word_verse``, ``lexicon``, ``clause_atom``.\n",
    "\n",
    "The tables ``book``, ``chapter``, ``verse``, ``clause_atom`` contain fields ``first_m``, ``last_m``, \n",
    "denoting the first and last monad number of that book, chapter, verse, clause_atom.\n",
    "\n",
    "A ``book``-record contains an identifier and the name of the book.\n",
    "\n",
    "A ``chapter``-record contains an identifier, the number of the chapter, and a foreign key to the record in the ``book`` table to which the chapter belongs.\n",
    "\n",
    "A ``verse``-record contains an identifier, the number of the verse, and a foreign key to the record in the ``chapter`` table to which the verse belongs. More over, it contains the text of the whole verse in two formats:\n",
    "\n",
    "In field ``text``: the plain Unicode text string of the complete verse.\n",
    "\n",
    "In field ``xml``: a sequence of ``<w>`` elements, one for each word in the verse, containing the plain Unicode text string of that word as element content.\n",
    "The monad number of that word is stored in an attribute value. \n",
    "The monad number is a globally unique sequence number of a word occurrence in the Hebrew Bible, going from 1 to precisely 426,555.\n",
    "There is also a lexical identifier stored in an attribute value.\n",
    "The lexical identifier points to the lexical entry that corresponds with the word.\n",
    "\n",
    "    <w m=\"2\" l=\"3\">רֵאשִׁ֖ית </w>\n",
    "\n",
    "As you see, the material between a word and the next word is appended to the first word. So, when you concatenate words, whitespace or other separators are needed.\n",
    "\n",
    "A ``word_verse``-record links a word to a verse. \n",
    "The monad number is in field ``anchor``, which is an integer, \n",
    "and the verse is specified in the field ``verse_id`` as foreign key.\n",
    "The field ``lexicon_id`` is a foreign key into the ``lexicon`` table.\n",
    "\n",
    "There is also a ``word`` table, meant to store all the information to generate a rich representation of the Hebrew text,\n",
    "its syntactic structure, and some linguistic properties.\n",
    "See that notebook for a description and an example of the rich Hebrew text representation.\n",
    "\n",
    "The rich data is added per word, but the data has a dependency on the verses the words are contained in.\n",
    "In general, information about sentences, clauses and phrases will be displayed on the first words of those objects,\n",
    "but if the object started in a previous verse, this information is repeated on the first word of that object in the\n",
    "current verse.\n",
    "This insures that the display of a verse is always self-contained.\n",
    "\n",
    "The ``word`` table has no field ``id``, its primary key is the field called ``word_number``. \n",
    "This fields contains the same monad number as is used in the field ``anchor`` of the table ``word_verse``.\n",
    "\n",
    "A ``clause_atom`` record contains an identifier, and the book to which it belongs, and its sequence number within \n",
    "that book.\n",
    "In SHEBANQ, manual annotations are linked to the clause atom, so we need this information to easily fetch comments to\n",
    "passages and to compose charts and CSV files.\n",
    "\n",
    "## Lexicon\n",
    "\n",
    "A ``lexicon`` record contains the various lexical fields, such as identifiers, entry representations,\n",
    "additional lexical properties, and a gloss.\n",
    "\n",
    "We make sure that we translate lexical feature values into values used for the BHSA.\n",
    "We need the following information per entry:\n",
    "\n",
    "* **id** a fresh id (see below), to be used in applications, unique over **entryid** and **lan**\n",
    "* **lan** the language of the entry, in ISO 639-3 abbreviation\n",
    "* **entryid** the string used as entry in the lexicon and as value of the ``lex`` feature in the text\n",
    "* **g_entryid** the Hebrew un-transliteration of entryid, with the disambiguation marks unchanged, corresponds to the ``lex_utf8`` feature\n",
    "* **entry** the unpointed transliteration (= **entryid** without disambiguation marks)\n",
    "* **entry_heb** the unpointed hebrew representation, obtained by un-transliterating **entry**\n",
    "* **g_entry** the pointed transliteration, without disambiguation marks, obtained from ``vc``\n",
    "* **g_entry_heb** the pointed hebrew representation, obtained by un-transliterating **g_entry**\n",
    "* **root** the root, obtained from ``rt``\n",
    "* **pos** the part of speech, obtained from ``sp``\n",
    "* **nametype** the type of named entity, obtained from ``sm``\n",
    "* **subpos** subtype of part of speech, obtained from ``ls`` (aka *lexical set*)\n",
    "* **gloss** the gloss from ``gl``\n",
    "\n",
    "We construct the **id** from the ``lex`` feature as follows:\n",
    "\n",
    "* allocate a varchar(32)\n",
    "* the > is an alef, we translate it to A\n",
    "* the < is an ayin, we translate it to O\n",
    "* the / denotes a noun, we translate it to n\n",
    "* the \\[ denotes a verb, we translate it to v\n",
    "* the = is for disambiguation, we translate it to i\n",
    "* we prepend a language identifier, 1 for Hebrew, 2 for Aramaic.\n",
    "\n",
    "This is sound, see the scheck in the extradata/lexicon notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Field transformation\n",
    "\n",
    "The lexical fields require a bit of attention.\n",
    "The specification in ``lexFields`` below specifies the lexicon fields in the intended order.\n",
    "It contains instructions how to construct the field values from the lexical information obtained from the lexicon files.\n",
    "\n",
    "    (source, method, name, transformation table, data type, data size, data options, params)\n",
    "\n",
    "## source \n",
    "May contain one of the following:\n",
    "\n",
    "* the name of a lexical feature as shown in the lexicon files, such as ``sp``, ``vc``.\n",
    "* None. \n",
    "  In this case, **method** is a code that triggers special actions, such as getting an id or something that is available to the   program that fills the lexicon table\n",
    "* the name of an other field as shown in the **name** part of the specification. \n",
    "  In this case, **method** is a function, defined else where, that takes the value of that other field as argument. \n",
    "  The function is typically a transliteration, or a stripping action.\n",
    "\n",
    "## method\n",
    "May contain one of the following:\n",
    "\n",
    "* a code (string), indicating:\n",
    "    * ``lex``: take the value of a feature (indicated in **source**) for this entry from the lexicon file\n",
    "    * ``entry``: take the value of the entry itself as found in the lexicon file\n",
    "    * ``id``: take the id for this entry as generated by the program\n",
    "    * ``lan``: take the language of this entry\n",
    "* a function taking one argument\n",
    "    * *strip_id*: strip the non-lexeme characters at the end of the entry (the ``/ [ =`` characters)\n",
    "    * *toHeb*: transform the transliteration into real Unicode Hebrew\n",
    "    * feature lookup functions such as ``F.lex.v``\n",
    "\n",
    "## name\n",
    "The name of the field in the to be constructed table ``lexicon`` in the database ``passage``.\n",
    "\n",
    "## data type\n",
    "The sql data type, such as ``int`` or ``varchar``, without the size and options.\n",
    "\n",
    "## data size\n",
    "The sql data size, which shows up between ``()`` after the data type\n",
    "\n",
    "## data options\n",
    "Any remaining type specification, such as `` character set utf8``.\n",
    "\n",
    "## params\n",
    "Params consists currently of 1 boolean, indicating whether the field is defined on all words of the object, or only on its first word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index of lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|         33s Lexicon arc has   707 entries\n",
      "|         33s Lexicon hbo has  8518 entries\n"
     ]
    }
   ],
   "source": [
    "lexEntries = {}\n",
    "\n",
    "for w in F.otype.s('word'):\n",
    "    lan = Fs(LANGUAGE).v(w)\n",
    "    lex = F.lex.v(w)\n",
    "    lex_utf8 = F.lex_utf8.v(w)\n",
    "    if lan in lexEntries and lex in lexEntries[lan]: continue\n",
    "\n",
    "    lexId = '{}{}'.format(\n",
    "        '1' if lan == 'hbo' else '2',\n",
    "        lex.\n",
    "            replace('>','A').\n",
    "            replace('<','O').\n",
    "            replace('[','v').\n",
    "            replace('/','n').\n",
    "            replace('=','i'),\n",
    "    )\n",
    "\n",
    "\n",
    "    lex0 = lex.rstrip('[/=]')\n",
    "    lexDis = '' if lex0 == lex else lex[len(lex0)-len(lex):]\n",
    "    \n",
    "    refNode = L.u(w, otype='lex')[0]\n",
    "    \n",
    "    vocLexNode = w if ENTRY == 'g_entry' else refNode\n",
    "    \n",
    "    voc_lex = Fs(ENTRY).v(vocLexNode)\n",
    "    voc_lex_utf8 = Fs(ENTRY_HEB).v(vocLexNode)\n",
    "    \n",
    "    root = F.root.v(refNode)\n",
    "    sp = F.sp.v(refNode)\n",
    "    nametype = F.nametype.v(refNode)\n",
    "    ls = F.ls.v(refNode)\n",
    "    gloss = F.gloss.v(refNode)\n",
    "    \n",
    "    lexEntries.setdefault(lan, {})[lex] = dict(\n",
    "        id=lexId,\n",
    "        lan=lan,\n",
    "        entryid=lex,\n",
    "        entry=lex0,\n",
    "        entry_heb=lex_utf8,\n",
    "        entryid_heb=lex_utf8+lexDis,\n",
    "        g_entry=voc_lex,\n",
    "        g_entry_heb=voc_lex_utf8,\n",
    "        root=root if root != None else '',\n",
    "        pos=sp if sp != None else '',\n",
    "        nametype=nametype if nametype != None else '',\n",
    "        subpos=ls if ls != None else '',\n",
    "        gloss=gloss if gloss != None else '',\n",
    "\n",
    "    )\n",
    "\n",
    "for lan in sorted(lexEntries):\n",
    "    utils.caption(0, 'Lexicon {} has {:>5} entries'.format(lan, len(lexEntries[lan])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index of ketiv/qere\n",
    "\n",
    "We make a list of the ketiv-qere items.\n",
    "It will be used by the *heb* and the *ktv* functions.\n",
    "\n",
    "*heb()* provides the surface text of a word.\n",
    "When the qere is different from the ketiv, the vocalized qere is chosen.\n",
    "It is the value of ``g_word_utf8`` except when a qere is present, \n",
    "in which case it is ``g_qere_utf8``, preceded by a masora circle.\n",
    "This is the sign for the user to use data view to inspect the *ketiv*.\n",
    "\n",
    "*ktv()* provides the surface text of a word, in case the ketiv is different from the qere.\n",
    "It is the value of ``g_word_utf8`` precisely when a qere is present, \n",
    "otherwise it is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|         38s Building qere index\n",
      "|         39s Found 426555 qeres\n"
     ]
    }
   ],
   "source": [
    "qeres = {}\n",
    "masora = '֯'\n",
    "utils.caption(0, 'Building qere index')\n",
    "for w in F.otype.s('word'):\n",
    "    q = Fs(QERE).v(w)\n",
    "    if q != None:\n",
    "        qeres[w] = (masora+q, Fs(QERE_TRAILER).v(w))\n",
    "utils.caption(0, 'Found {} qeres'.format(len(qeres)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index of paragraphs\n",
    "\n",
    "We make a list of paragraph numbers of clause_atoms. It will be used by the *para* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|         43s Building para index\n",
      "|         43s Found para information for 90144 clause_atoms\n"
     ]
    }
   ],
   "source": [
    "paras = {}\n",
    "utils.caption(0, 'Building para index')\n",
    "for c in F.otype.s('clause_atom'):\n",
    "    par = F.pargr.v(c)\n",
    "    paras[c] = par\n",
    "utils.caption(0, 'Found para information for {} clause_atoms'.format(len(paras)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Field types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_id(entryid):\n",
    "    return entryid.rstrip('/[=')\n",
    "\n",
    "def toHeb(translit):\n",
    "    return Transcription.toHebrew(Transcription.suffix_and_finales(translit)[0])\n",
    "\n",
    "def ide(n): return n\n",
    "\n",
    "def heb(n):\n",
    "    if n in qeres:\n",
    "        (trsep, wrdrep) = qeres[n]\n",
    "    else:\n",
    "        trsep = F.trailer_utf8.v(n)\n",
    "        wrdrep = F.g_word_utf8.v(n)\n",
    "    if trsep.endswith('ס') or trsep.endswith('פ'): trsep += ' '\n",
    "    return wrdrep + trsep\n",
    "\n",
    "def ktv(n):\n",
    "    if n in qeres:\n",
    "        trsep = F.trailer_utf8.v(n)\n",
    "        if trsep.endswith('ס') or trsep.endswith('פ'): trsep += ' '\n",
    "        return F.g_word_utf8.v(n) + trsep    \n",
    "    return ''\n",
    "\n",
    "def para(n): return paras.get(n, '')\n",
    "\n",
    "def lang(n):\n",
    "    return Fs(LANGUAGE).v(n)\n",
    "\n",
    "def df(f):\n",
    "    def g(n): \n",
    "        val = f(n)\n",
    "#        if val == None or val == \"None\" or val == \"none\" or val == \"NA\" or val == \"N/A\" or val == \"n/a\":\n",
    "        if val == None:\n",
    "            return '#null'\n",
    "        return val\n",
    "    return g\n",
    "\n",
    "def dfl(f):\n",
    "    def g(n): \n",
    "        val = f(L.u(n, otype='lex')[0])\n",
    "#        if val == None or val == \"None\" or val == \"none\" or val == \"NA\" or val == \"N/A\" or val == \"n/a\":\n",
    "        if val == None:\n",
    "            return 'NA'\n",
    "        return val\n",
    "    return g\n",
    "\n",
    "lexFields = (\n",
    "    ('id', 'varchar', 32, ' primary key'),\n",
    "    ('lan', 'char', 4, ''),\n",
    "    ('entryid', 'varchar', 32, ''),\n",
    "    ('entry', 'varchar', 32, ''),\n",
    "    ('entry_heb', 'varchar', 32, ' character set utf8'),\n",
    "    ('entryid_heb', 'varchar', 32, ' character set utf8'),\n",
    "    ('g_entry', 'varchar', 32, ''),\n",
    "    ('g_entry_heb', 'varchar', 32, ' character set utf8'),\n",
    "    ('root', 'varchar', 32, ''),\n",
    "    ('pos', 'varchar', 8, ''),\n",
    "    ('nametype', 'varchar', 16, ''),\n",
    "    ('subpos', 'varchar', 8, ''),\n",
    "    ('gloss', 'varchar', 32, ' character set utf8'),\n",
    ")\n",
    "wordFields = (\n",
    "    (ide, 'number', 'word', 'int', 4, ' primary key', False),\n",
    "    (heb, 'heb', 'word', 'varchar', 32, ' character set utf8', False),\n",
    "    (ktv, 'ktv', 'word', 'varchar', 32, ' character set utf8', False),\n",
    "    (dfl(Fs(ENTRY_HEB).v), 'vlex', 'word', 'varchar', 32, ' character set utf8', False),\n",
    "    (F.lex_utf8.v, 'clex', 'word', 'varchar', 32, ' character set utf8', False),\n",
    "    (F.g_word.v, 'tran', 'word', 'varchar', 32, ' character set utf8', False),\n",
    "    (F.phono.v, 'phono', 'word', 'varchar', 32, ' character set utf8', False),\n",
    "    (Fs(PHONO_TRAILER).v, 'phono_sep', 'word', 'varchar', 8, ' character set utf8', False),\n",
    "    (F.lex.v, 'lex', 'word', 'varchar', 32, ' character set utf8', False),\n",
    "    (F.g_lex.v, 'glex', 'word', 'varchar', 32, ' character set utf8', False),\n",
    "    (dfl(F.gloss.v), 'gloss', 'word', 'varchar', 32, ' character set utf8', False),\n",
    "    (lang, 'lang', 'word', 'varchar', 8, ' character set utf8', False),\n",
    "    (df(F.sp.v), 'pos', 'word', 'varchar', 8, ' character set utf8', False),\n",
    "    (df(F.pdp.v), 'pdp', 'word', 'varchar', 8, ' character set utf8', False),\n",
    "    (df(F.ls.v), 'subpos', 'word', 'varchar', 8, ' character set utf8', False),\n",
    "    (dfl(F.nametype.v), 'nmtp', 'word', 'varchar', 32, ' character set utf8', False),\n",
    "    (df(F.vt.v), 'tense', 'word', 'varchar', 8, ' character set utf8', False),\n",
    "    (df(F.vs.v), 'stem', 'word', 'varchar', 8, ' character set utf8', False),\n",
    "    (df(F.gn.v), 'gender', 'word', 'varchar', 8, ' character set utf8', False),\n",
    "    (df(F.nu.v), 'gnumber', 'word', 'varchar', 8, ' character set utf8', False),\n",
    "    (df(F.ps.v), 'person', 'word', 'varchar', 8, ' character set utf8', False),\n",
    "    (df(F.st.v), 'state', 'word', 'varchar', 8, ' character set utf8', False),\n",
    "    (df(F.nme.v), 'nme', 'word', 'varchar', 8, ' character set utf8', False),\n",
    "    (df(F.pfm.v), 'pfm', 'word', 'varchar', 8, ' character set utf8', False),\n",
    "    (df(F.prs.v), 'prs', 'word', 'varchar', 8, ' character set utf8', False),\n",
    "    (df(F.uvf.v), 'uvf', 'word', 'varchar', 8, ' character set utf8', False),\n",
    "    (df(F.vbe.v), 'vbe', 'word', 'varchar', 8, ' character set utf8', False),\n",
    "    (df(F.vbs.v), 'vbs', 'word', 'varchar', 8, ' character set utf8', False),\n",
    "    (F.freq_lex.v, 'freq_lex', 'word', 'int', 4, '', False),\n",
    "    (F.freq_occ.v, 'freq_occ', 'word', 'int', 4, '', False),\n",
    "    (F.rank_lex.v, 'rank_lex', 'word', 'int', 4, '', False),\n",
    "    (F.rank_occ.v, 'rank_occ', 'word', 'int', 4, '', False),\n",
    "    (None, 'border', 'subphrase', 'varchar', 16, ' character set utf8', False),\n",
    "    ('id', 'number', 'subphrase', 'varchar', 32, ' character set utf8', False),\n",
    "    (df(F.rela.v), 'rela', 'subphrase', 'varchar', 8, ' character set utf8', True),\n",
    "    (None, 'border', 'phrase', 'varchar', 8, ' character set utf8', False),\n",
    "    (F.number.v, 'number', 'phrase_atom', 'int', 4, '', False),\n",
    "    (df(F.rela.v), 'rela', 'phrase_atom', 'varchar', 8, ' character set utf8', True),\n",
    "    (F.number.v, 'number', 'phrase', 'int', 4, '', False),\n",
    "    (df(F.function.v), 'function', 'phrase', 'varchar', 8, ' character set utf8', True),\n",
    "    (df(F.rela.v), 'rela', 'phrase', 'varchar', 8, ' character set utf8', True),\n",
    "    (df(F.typ.v), 'typ', 'phrase', 'varchar', 8, ' character set utf8', True),\n",
    "    (df(F.det.v), 'det', 'phrase', 'varchar', 8, ' character set utf8', True),\n",
    "    (None, 'border', 'clause', 'varchar', 8, ' character set utf8', False),\n",
    "    (F.number.v, 'number', 'clause_atom', 'int', 4, '', False),\n",
    "    (df(F.code.v), 'code', 'clause_atom', 'int', 4, '', True),\n",
    "    (df(F.tab.v), 'tab', 'clause_atom', 'int', 4, '', False),\n",
    "    (para, 'pargr', 'clause_atom', 'varchar', 64, ' character set utf8', True),\n",
    "    (F.number.v, 'number', 'clause', 'int', 4, '', False),\n",
    "    (df(F.rela.v), 'rela', 'clause', 'varchar', 8, ' character set utf8', True),\n",
    "    (df(F.typ.v), 'typ', 'clause', 'varchar', 8, ' character set utf8', True),\n",
    "    (df(F.txt.v), 'txt', 'clause', 'varchar', 8, ' character set utf8', False),\n",
    "    (None, 'border', 'sentence', 'varchar', 8, ' character set utf8', False),\n",
    "    (F.number.v, 'number', 'sentence_atom', 'int', 4, '', False),\n",
    "    (F.number.v, 'number', 'sentence', 'int', 4, '', False),\n",
    ")\n",
    "firstOnly = dict(('{}_{}'.format(f[2], f[1]), f[6]) for f in wordFields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity\n",
    "The texts and XML representations of verses are stored in ``varchar`` fields.\n",
    "We have to make sure that the values fit within the declared sizes of these fields.\n",
    "The code measures the maximum lengths of these fields, and it turns out that the text is maximally 434 chars and the XML 2186 chars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "set character_set_client = 'utf8';\n",
      "set character_set_connection = 'utf8';\n",
      "\n",
      "drop database if exists shebanq_passage4;\n",
      "\n",
      "create database shebanq_passage4 character set utf8;\n",
      "\n",
      "use shebanq_passage4;\n",
      "\n",
      "create table book(\n",
      "    id      int(4) primary key,\n",
      "    first_m int(4),\n",
      "    last_m int(4),\n",
      "    name varchar(32),\n",
      "    index(name)\n",
      ");\n",
      "\n",
      "create table chapter(\n",
      "    id int(4) primary key,\n",
      "    first_m int(4),\n",
      "    last_m int(4),\n",
      "    book_id int(4),\n",
      "    chapter_num int(4),\n",
      "    foreign key (book_id) references book(id),\n",
      "    index(chapter_num)\n",
      ");\n",
      "\n",
      "create table verse(\n",
      "    id int(4) primary key,\n",
      "    first_m int(4),\n",
      "    last_m int(4),\n",
      "    chapter_id int(4),\n",
      "    verse_num int(4),\n",
      "    text varchar(1024) character set utf8,\n",
      "    xml varchar(4096) character set utf8,\n",
      "    foreign key (chapter_id) references chapter(id)\n",
      ");\n",
      "\n",
      "create table clause_atom(\n",
      "    id int(4) primary key,\n",
      "    first_m int(4),\n",
      "    last_m int(4),\n",
      "    ca_num int(4),    \n",
      "    book_id int(4),\n",
      "    text varchar(512) character set utf8,\n",
      "    foreign key (book_id) references book(id),\n",
      "    index(ca_num)\n",
      ");\n",
      "\n",
      "create table word(\n",
      "    word_number int(4) primary key, \n",
      "    word_heb varchar(32) character set utf8, \n",
      "    word_ktv varchar(32) character set utf8, \n",
      "    word_vlex varchar(32) character set utf8, \n",
      "    word_clex varchar(32) character set utf8, \n",
      "    word_tran varchar(32) character set utf8, \n",
      "    word_phono varchar(32) character set utf8, \n",
      "    word_phono_sep varchar(8) character set utf8, \n",
      "    word_lex varchar(32) character set utf8, \n",
      "    word_glex varchar(32) character set utf8, \n",
      "    word_gloss varchar(32) character set utf8, \n",
      "    word_lang varchar(8) character set utf8, \n",
      "    word_pos varchar(8) character set utf8, \n",
      "    word_pdp varchar(8) character set utf8, \n",
      "    word_subpos varchar(8) character set utf8, \n",
      "    word_nmtp varchar(32) character set utf8, \n",
      "    word_tense varchar(8) character set utf8, \n",
      "    word_stem varchar(8) character set utf8, \n",
      "    word_gender varchar(8) character set utf8, \n",
      "    word_gnumber varchar(8) character set utf8, \n",
      "    word_person varchar(8) character set utf8, \n",
      "    word_state varchar(8) character set utf8, \n",
      "    word_nme varchar(8) character set utf8, \n",
      "    word_pfm varchar(8) character set utf8, \n",
      "    word_prs varchar(8) character set utf8, \n",
      "    word_uvf varchar(8) character set utf8, \n",
      "    word_vbe varchar(8) character set utf8, \n",
      "    word_vbs varchar(8) character set utf8, \n",
      "    word_freq_lex int(4), \n",
      "    word_freq_occ int(4), \n",
      "    word_rank_lex int(4), \n",
      "    word_rank_occ int(4), \n",
      "    subphrase_border varchar(16) character set utf8, \n",
      "    subphrase_number varchar(32) character set utf8, \n",
      "    subphrase_rela varchar(8) character set utf8, \n",
      "    phrase_border varchar(8) character set utf8, \n",
      "    phrase_atom_number int(4), \n",
      "    phrase_atom_rela varchar(8) character set utf8, \n",
      "    phrase_number int(4), \n",
      "    phrase_function varchar(8) character set utf8, \n",
      "    phrase_rela varchar(8) character set utf8, \n",
      "    phrase_typ varchar(8) character set utf8, \n",
      "    phrase_det varchar(8) character set utf8, \n",
      "    clause_border varchar(8) character set utf8, \n",
      "    clause_atom_number int(4), \n",
      "    clause_atom_code int(4), \n",
      "    clause_atom_tab int(4), \n",
      "    clause_atom_pargr varchar(64) character set utf8, \n",
      "    clause_number int(4), \n",
      "    clause_rela varchar(8) character set utf8, \n",
      "    clause_typ varchar(8) character set utf8, \n",
      "    clause_txt varchar(8) character set utf8, \n",
      "    sentence_border varchar(8) character set utf8, \n",
      "    sentence_atom_number int(4), \n",
      "    sentence_number int(4)\n",
      ");\n",
      "\n",
      "create table lexicon(\n",
      "    id varchar(32) primary key,\n",
      "    lan char(4),\n",
      "    entryid varchar(32),\n",
      "    entry varchar(32),\n",
      "    entry_heb varchar(32) character set utf8,\n",
      "    entryid_heb varchar(32) character set utf8,\n",
      "    g_entry varchar(32),\n",
      "    g_entry_heb varchar(32) character set utf8,\n",
      "    root varchar(32),\n",
      "    pos varchar(8),\n",
      "    nametype varchar(16),\n",
      "    subpos varchar(8),\n",
      "    gloss varchar(32) character set utf8    \n",
      ") collate utf8_bin;\n",
      "\n",
      "create table word_verse(\n",
      "    anchor int(4) unique,\n",
      "    verse_id int(4),\n",
      "    lexicon_id varchar(32),\n",
      "    foreign key (anchor) references word(word_number),\n",
      "    foreign key (verse_id) references verse(id),\n",
      "    foreign key (lexicon_id) references lexicon(id)\n",
      ") collate utf8_bin;\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fieldLimits = {\n",
    "    'book': {\n",
    "        'name': 32,\n",
    "    },\n",
    "    'verse': {\n",
    "        'text': 1024,\n",
    "        'xml': 4096,\n",
    "    },\n",
    "    'clause_atom': {\n",
    "        'text': 512,\n",
    "    },\n",
    "    'lexicon': {},\n",
    "}\n",
    "for f in lexFields:\n",
    "    if f[1].endswith('char'):\n",
    "        fieldLimits['lexicon'][f[0]] = f[2]\n",
    "\n",
    "config = {\n",
    "    'db': 'shebanq_passage'+VERSION,\n",
    "}\n",
    "for tb in fieldLimits:\n",
    "    for fl in fieldLimits[tb]: config['{}_{}'.format(tb, fl)] = fieldLimits[tb][fl]\n",
    "\n",
    "textCreateSql = '''\n",
    "set character_set_client = 'utf8';\n",
    "set character_set_connection = 'utf8';\n",
    "\n",
    "drop database if exists {db};\n",
    "\n",
    "create database {db} character set utf8;\n",
    "\n",
    "use {db};\n",
    "\n",
    "create table book(\n",
    "    id      int(4) primary key,\n",
    "    first_m int(4),\n",
    "    last_m int(4),\n",
    "    name varchar({book_name}),\n",
    "    index(name)\n",
    ");\n",
    "\n",
    "create table chapter(\n",
    "    id int(4) primary key,\n",
    "    first_m int(4),\n",
    "    last_m int(4),\n",
    "    book_id int(4),\n",
    "    chapter_num int(4),\n",
    "    foreign key (book_id) references book(id),\n",
    "    index(chapter_num)\n",
    ");\n",
    "\n",
    "create table verse(\n",
    "    id int(4) primary key,\n",
    "    first_m int(4),\n",
    "    last_m int(4),\n",
    "    chapter_id int(4),\n",
    "    verse_num int(4),\n",
    "    text varchar({verse_text}) character set utf8,\n",
    "    xml varchar({verse_xml}) character set utf8,\n",
    "    foreign key (chapter_id) references chapter(id)\n",
    ");\n",
    "\n",
    "create table clause_atom(\n",
    "    id int(4) primary key,\n",
    "    first_m int(4),\n",
    "    last_m int(4),\n",
    "    ca_num int(4),    \n",
    "    book_id int(4),\n",
    "    text varchar({clause_atom_text}) character set utf8,\n",
    "    foreign key (book_id) references book(id),\n",
    "    index(ca_num)\n",
    ");\n",
    "\n",
    "create table word(\n",
    "    {{wordfields}}\n",
    ");\n",
    "\n",
    "create table lexicon(\n",
    "    {{lexfields}}    \n",
    ") collate utf8_bin;\n",
    "\n",
    "create table word_verse(\n",
    "    anchor int(4) unique,\n",
    "    verse_id int(4),\n",
    "    lexicon_id varchar(32),\n",
    "    foreign key (anchor) references word(word_number),\n",
    "    foreign key (verse_id) references verse(id),\n",
    "    foreign key (lexicon_id) references lexicon(id)\n",
    ") collate utf8_bin;\n",
    "\n",
    "'''.format(**config).format(\n",
    "        lexfields = ',\\n    '.join('{} {}({}){}'.format(\n",
    "            f[0], f[1], f[2], f[3],\n",
    "        ) for f in lexFields),\n",
    "        wordfields = ', \\n    '.join('{}_{} {}({}){}'.format(\n",
    "            f[2], f[1], f[3], f[4], f[5],\n",
    "    ) for f in wordFields),\n",
    ")\n",
    "if not SCRIPT:\n",
    "    print(textCreateSql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table filling\n",
    "\n",
    "We compose all the records for all the tables.\n",
    "\n",
    "We also generate a file that can act as the basis of an extra annotation file with lexical information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................\n",
      ".      1m 49s Fill the tables ...                                                            .\n",
      "..............................................................................................\n",
      "|      1m 53s OK    book           name           : max size =      13 of    32\n",
      "|      1m 53s OK    clause_atom    text           : max size =     261 of   512\n",
      "|      1m 53s OK    lexicon        entry          : max size =      14 of    32\n",
      "|      1m 53s OK    lexicon        entry_heb      : max size =      15 of    32\n",
      "|      1m 53s OK    lexicon        entryid        : max size =      15 of    32\n",
      "|      1m 53s OK    lexicon        entryid_heb    : max size =      21 of    32\n",
      "|      1m 53s OK    lexicon        g_entry        : max size =      32 of    32\n",
      "|      1m 53s OK    lexicon        g_entry_heb    : max size =      31 of    32\n",
      "|      1m 53s OK    lexicon        gloss          : max size =      26 of    32\n",
      "|      1m 53s OK    lexicon        id             : max size =      16 of    32\n",
      "|      1m 53s OK    lexicon        lan            : max size =       3 of     4\n",
      "|      1m 53s OK    lexicon        nametype       : max size =      14 of    16\n",
      "|      1m 53s OK    lexicon        pos            : max size =       4 of     8\n",
      "|      1m 53s OK    lexicon        root           : max size =       4 of    32\n",
      "|      1m 53s OK    lexicon        subpos         : max size =       4 of     8\n",
      "|      1m 53s OK    verse          text           : max size =      94 of  1024\n",
      "|      1m 53s OK    verse          xml            : max size =    2498 of  4096\n",
      "|      1m 53s Done\n",
      "All lexemes have been found in the lexicon\n"
     ]
    }
   ],
   "source": [
    "utils.caption(4, 'Fill the tables ... ')\n",
    "curId = {\n",
    "    'book': -1,\n",
    "    'chapter': - 1,\n",
    "    'verse': -1,\n",
    "    'clause_atom': -1\n",
    "}\n",
    "\n",
    "def sEsc(sql): return sql.replace(\"'\", \"''\").replace('\\\\','\\\\\\\\').replace('\\n','\\\\n')\n",
    "\n",
    "curVerseNode = None\n",
    "curVerseInfo = []\n",
    "curVerseFirstSlot = None\n",
    "curVerseLastSlot = None\n",
    "curLexValues = {}\n",
    "\n",
    "lexIndex = {}\n",
    "lexNotFound = collections.defaultdict(lambda: collections.Counter())\n",
    "tables = collections.defaultdict(lambda: [])\n",
    "fieldSizes = collections.defaultdict(lambda: collections.defaultdict(lambda: 0))\n",
    "\n",
    "Fotypev = F.otype.v\n",
    "Ftextv = F.g_word_utf8.v\n",
    "Foccv = F.g_cons.v\n",
    "Flexv = F.lex.v\n",
    "Flanguagev = Fs(LANGUAGE).v\n",
    "Ftrailerv = F.trailer_utf8.v\n",
    "Fnumberv = F.number.v\n",
    "\n",
    "def computeFields(entryData):\n",
    "    eId = entryData['id']\n",
    "    lan = entryData['lan']\n",
    "    entryid = entryData['entryid']\n",
    "    lexIndex[(lan, entryid)] = eId\n",
    "    result = []\n",
    "    for (fName, fType, fSize, fRest) in lexFields:\n",
    "        val = entryData[fName]\n",
    "        val = 'null' if val == None else \\\n",
    "            val if fType == 'int' else \\\n",
    "            \"'{}'\".format(sEsc(val))\n",
    "        if fName in fieldLimits['lexicon']:\n",
    "            fieldSizes['lexicon'][fName] = max(len(val)-2, fieldSizes['lexicon'][fName])\n",
    "        result.append(val)        \n",
    "    return result\n",
    "\n",
    "for lan in sorted(lexEntries):\n",
    "    for (entry, entryData) in sorted(lexEntries[lan].items()):\n",
    "        format_str = '({})'.format(','.join('{}' for f in lexFields))\n",
    "        entryInfo = computeFields(entryData)\n",
    "        tables['lexicon'].append(format_str.format(\n",
    "            *entryInfo\n",
    "        ))\n",
    "\n",
    "def doVerse(node):\n",
    "    global curVerseNode, curVerseInfo, max_len_text, max_len_xml\n",
    "    if curVerseNode != None:\n",
    "        thisText = ''.join('{}{}'.format(x[0], x[1]) for x in curVerseInfo)\n",
    "        thisXml = ''.join(\n",
    "            '''<w m=\"{}\" t=\"{}\" l=\"{}\">{}</w>'''.format(\n",
    "                x[2], x[1].replace('\\n', '&#xa;'), x[4], x[0]\n",
    "            ) for x in curVerseInfo)\n",
    "        fieldSizes['verse']['text'] = max((len(thisText), fieldSizes['verse']['text']))\n",
    "        fieldSizes['verse']['xml'] = max((len(thisXml), fieldSizes['verse']['xml']))\n",
    "        tables['verse'].append(\"({},{},{},{},{},'{}','{}')\".format(\n",
    "            curId['verse'], \n",
    "            curVerseFirstSlot, \n",
    "            curVerseLastSlot, \n",
    "            curId['chapter'], F.verse.v(curVerseNode), sEsc(thisText), sEsc(thisXml),\n",
    "        ))\n",
    "        for x in curVerseInfo:\n",
    "            tables['word_verse'].append(\"({}, {}, '{}')\".format(\n",
    "                x[2], x[3], x[4]\n",
    "            ))\n",
    "        curVerseInfo = []\n",
    "    curVerseNode = node    \n",
    "\n",
    "for node in N():\n",
    "    otype = Fotypev(node)\n",
    "    if otype == 'word':\n",
    "        if node in qeres:\n",
    "            (text, trailer) = qeres[node]\n",
    "        else:\n",
    "            text = Ftextv(node)\n",
    "            trailer = Ftrailerv(node)\n",
    "        if trailer.endswith('ס') or trailer.endswith('פ'): trailer += ' '\n",
    "        lex = Flexv(node)\n",
    "        lang = Flanguagev(node)\n",
    "        lid = lexIndex.get((lang, lex), None)\n",
    "        if lid == None:\n",
    "            lexNotFound[(lang, lex)][Foccv(node)] += 1\n",
    "        curVerseInfo.append((\n",
    "            text,\n",
    "            trailer,\n",
    "            node, \n",
    "            curId['verse'],\n",
    "            lid,\n",
    "        ))\n",
    "    elif otype == 'verse':\n",
    "        doVerse(node)\n",
    "        slots = L.d(node, otype='word')\n",
    "        curId['verse'] += 1\n",
    "        curVerseFirstSlot = slots[0]\n",
    "        curVerseLastSlot = slots[-1]\n",
    "    elif otype == 'chapter':\n",
    "        doVerse(None)\n",
    "        slots = L.d(node, otype='word')\n",
    "        curId['chapter'] += 1\n",
    "        tables['chapter'].append(\"({},{},{},{},{})\".format(\n",
    "            curId['chapter'], slots[0], slots[-1], curId['book'], F.chapter.v(node),\n",
    "        ))\n",
    "    elif otype == 'book':\n",
    "        doVerse(None)\n",
    "        slots = L.d(node, otype='word')\n",
    "        curId['book'] += 1\n",
    "        name = F.book.v(node)\n",
    "        fieldSizes['book']['name'] = max((len(name), fieldSizes['book']['name']))\n",
    "        tables['book'].append(\"({},{},{},'{}')\".format(\n",
    "            curId['book'], slots[0], slots[-1], sEsc(name),\n",
    "        ))\n",
    "    elif otype == 'clause_atom':\n",
    "        curId['clause_atom'] += 1\n",
    "        slots = L.d(node, otype='word')\n",
    "        ca_num = Fnumberv(node)\n",
    "        wordtexts = []\n",
    "        for w in L.d(node, otype='word'):\n",
    "            trsep = Ftrailerv(w)\n",
    "            if trsep.endswith('ס') or trsep.endswith('פ'): trsep += ' '\n",
    "            wordtexts.append(F.g_word_utf8.v(w) +trsep)\n",
    "        text = ''.join(wordtexts)\n",
    "        fieldSizes['clause_atom']['text'] = max((len(text), fieldSizes['clause_atom']['text']))\n",
    "        tables['clause_atom'].append(\"({},{},{},{},{},'{}')\".format(\n",
    "            curId['clause_atom'], slots[0], slots[-1], ca_num, curId['book'], sEsc(text),\n",
    "        ))\n",
    "doVerse(None)\n",
    "\n",
    "for tb in sorted(fieldLimits):\n",
    "    for fl in sorted(fieldLimits[tb]):\n",
    "        limit = fieldLimits[tb][fl]\n",
    "        actual = fieldSizes[tb][fl]\n",
    "        exceeded = actual > limit\n",
    "        utils.caption(0, '{:<5} {:<15}{:<15}: max size = {:>7} of {:>5}'.format(\n",
    "            'ERROR' if exceeded else 'OK',\n",
    "            tb, fl, actual, limit,\n",
    "        ))\n",
    "\n",
    "utils.caption(0, 'Done')\n",
    "if len(lexNotFound):\n",
    "    utils.caption(0, 'Text lexemes not found in lexicon: {}x'.format(len(lexNotFound)))\n",
    "    for l in sorted(lexNotFound):\n",
    "        utils.caption(0, '{} {}'.format(*l))\n",
    "        for (o, n) in sorted(lexNotFound[l].items(), key=lambda x: (-x[1], x[0])):\n",
    "            utils.caption(0, '\\t{}: {}x'.format(o, n))\n",
    "else:\n",
    "    print('All lexemes have been found in the lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill the word info table with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetTypes = {\n",
    "    'sentence', 'sentence_atom', \n",
    "    'clause', 'clause_atom', \n",
    "    'phrase', 'phrase_atom', \n",
    "    'subphrase',\n",
    "}\n",
    "\n",
    "def ranges(slotSet):\n",
    "    result = []\n",
    "    curStart = None\n",
    "    curEnd = None\n",
    "    for i in sorted(slotSet):\n",
    "        if curStart == None:\n",
    "            curStart = i\n",
    "            curEnd = i\n",
    "        else:\n",
    "            if i == curEnd + 1:\n",
    "                curEnd += 1\n",
    "            else:\n",
    "                result.append((curStart, curEnd))\n",
    "                curStart = i\n",
    "                curEnd = i\n",
    "    if curStart != None:\n",
    "        result.append((curStart, curEnd))\n",
    "    return result\n",
    "\n",
    "def getObjects(vn):\n",
    "    objects = set()\n",
    "    for wn in L.d(vn, otype='word'):\n",
    "        objects.add(wn)\n",
    "        for tt in targetTypes:\n",
    "            for on in L.u(wn, otype=tt):\n",
    "                objects.add(on)\n",
    "    return objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................\n",
      ".      1m 59s Generating word info data ...                                                  .\n",
      "..............................................................................................\n",
      "|      1m 59s \tGenesis\n",
      "|      2m 08s \tExodus\n",
      "|      2m 15s \tLeviticus\n",
      "|      2m 20s \tNumeri\n",
      "|      2m 27s \tDeuteronomium\n",
      "|      2m 33s \tJosua\n",
      "|      2m 38s \tJudices\n",
      "|      2m 42s \tSamuel_I\n",
      "|      2m 48s \tSamuel_II\n",
      "|      2m 52s \tReges_I\n",
      "|      2m 58s \tReges_II\n",
      "|      3m 03s \tJesaia\n",
      "|      3m 11s \tJeremia\n",
      "|      3m 20s \tEzechiel\n",
      "|      3m 28s \tHosea\n",
      "|      3m 29s \tJoel\n",
      "|      3m 30s \tAmos\n",
      "|      3m 30s \tObadia\n",
      "|      3m 31s \tJona\n",
      "|      3m 31s \tMicha\n",
      "|      3m 32s \tNahum\n",
      "|      3m 32s \tHabakuk\n",
      "|      3m 32s \tZephania\n",
      "|      3m 32s \tHaggai\n",
      "|      3m 33s \tSacharia\n",
      "|      3m 34s \tMaleachi\n",
      "|      3m 34s \tPsalmi\n",
      "|      3m 43s \tIob\n",
      "|      3m 46s \tProverbia\n",
      "|      3m 49s \tRuth\n",
      "|      3m 50s \tCanticum\n",
      "|      3m 50s \tEcclesiastes\n",
      "|      3m 52s \tThreni\n",
      "|      3m 52s \tEsther\n",
      "|      3m 54s \tDaniel\n",
      "|      3m 56s \tEsra\n",
      "|      3m 58s \tNehemia\n",
      "|      4m 00s \tChronica_I\n",
      "|      4m 05s \tChronica_II\n",
      "|      4m 11s Done\n"
     ]
    }
   ],
   "source": [
    "utils.caption(4, 'Generating word info data ...')\n",
    "tables['word'] = []\n",
    "\n",
    "if 'word' in fieldSizes: del fieldSizes['word']\n",
    "\n",
    "def doVerseInfo(verse):\n",
    "    slots = L.d(verse, otype='word')\n",
    "    \n",
    "    (verseStartSlot, verseEndSlot) = (slots[0], slots[-1])\n",
    "    objects = getObjects(verse)\n",
    "    words = [dict() for i in range(verseStartSlot, verseEndSlot + 1)]\n",
    "    for w in words:\n",
    "        for (otype, doBorder) in (\n",
    "            ('sentence', True), \n",
    "            ('sentence_atom', False), \n",
    "            ('clause', True), \n",
    "            ('clause_atom', False), \n",
    "            ('phrase', True),\n",
    "            ('phrase_atom', False),\n",
    "            ('subphrase', True),\n",
    "            ('word', False),\n",
    "        ):\n",
    "            w['{}_{}'.format(otype, 'number')] = list()\n",
    "            if doBorder:\n",
    "                w['{}_{}'.format(otype, 'border')] = set()\n",
    "    nWords = len(words)\n",
    "    subphraseCounter = 0\n",
    "    wordNodes = []\n",
    "    for n in objects:\n",
    "        otype = F.otype.v(n)\n",
    "        if otype == 'word': wordNodes.append(n)\n",
    "        numberProp = '{}_{}'.format(otype, 'number')\n",
    "        if otype != 'word' and not otype.endswith('_atom'):\n",
    "            borderProp = '{}_{}'.format(otype, 'border')\n",
    "        else:\n",
    "            borderProp = None\n",
    "\n",
    "        if otype == 'subphrase': subphraseCounter += 1\n",
    "        elif otype in {'phrase', 'clause', 'sentence'}: subphraseCounter = 0\n",
    "# Here was a bug: I put the subphraseCounter to 0 upon encountering anything else than a subphrase or a word.\n",
    "# I had overlooked the half_verse, which can cut through a phrase\n",
    "        thisInfo = {}\n",
    "        thisNumber = None\n",
    "        for f in wordFields:\n",
    "            (method, name, typ) = (f[0], '{}_{}'.format(f[2], f[1]), f[3])\n",
    "            if otype != f[2] or method == None: continue\n",
    "            if method == 'id':\n",
    "                value = subphraseCounter\n",
    "            else:\n",
    "                value = method(n)\n",
    "                if typ == 'int': value = int(value)\n",
    "            if name == numberProp:\n",
    "                thisNumber = value\n",
    "            else:\n",
    "                thisInfo[name] = value\n",
    "        if otype == 'word':\n",
    "            target = words[thisNumber - verseStartSlot]\n",
    "            target.update(thisInfo)\n",
    "            target[numberProp].append(thisNumber)            \n",
    "        else:\n",
    "            theseRanges = ranges(L.d(n, otype='word'))\n",
    "            nRanges = len(theseRanges) - 1\n",
    "            for (e,r) in enumerate(theseRanges):\n",
    "                isFirst = e == 0\n",
    "                isLast = e == nRanges\n",
    "                rightBorder = 'rr' if isFirst else 'r'\n",
    "                leftBorder = 'll' if isLast else 'l'\n",
    "                firstWord = -1 if r[0] < verseStartSlot else nWords if r[0] > verseEndSlot else r[0] - verseStartSlot\n",
    "                lastWord = -1 if r[1] < verseStartSlot else nWords if r[1] > verseEndSlot else r[1] - verseStartSlot\n",
    "                myFirstWord = max(firstWord, 0)\n",
    "                myLastWord = min(lastWord, nWords - 1)\n",
    "                for i in range(myFirstWord, myLastWord + 1):\n",
    "                    target = words[i]\n",
    "                    if not firstOnly[numberProp] or i == myFirstWord:\n",
    "                        target[numberProp].append(thisNumber)\n",
    "                    for f in thisInfo:\n",
    "                        if not firstOnly[name] or i == myFirstWord:\n",
    "                            target[f] = thisInfo[f]\n",
    "                    if otype == 'subphrase':\n",
    "                        if borderProp != None: words[i][borderProp].add('sy')\n",
    "                if 0 <= firstWord < nWords:\n",
    "                    if borderProp != None: words[firstWord][borderProp].add(rightBorder)\n",
    "                if 0 <= lastWord < nWords:\n",
    "                    if borderProp != None: words[lastWord][borderProp].add(leftBorder)\n",
    "    wordtext = []\n",
    "    for w in wordNodes:\n",
    "        trsep = Ftrailerv(w)\n",
    "        if trsep.endswith('ס') or trsep.endswith('פ'): trsep += ' '\n",
    "        wordtext.append(F.g_word_utf8.v(w) +trsep)\n",
    "    for w in words:\n",
    "        row = []\n",
    "        rrow = []\n",
    "        for f in wordFields:\n",
    "            typ = f[3]\n",
    "            name = '{}_{}'.format(f[2], f[1])\n",
    "            value = w.get(name, 'NULL' if typ == 'int' else '')\n",
    "            if f[1] == 'border':\n",
    "                value = ' '.join(value)\n",
    "            elif f[1] == 'number':\n",
    "                value = ' '.join(str(v) for v in value)\n",
    "            rrow.append(str(value).replace('\\n', '\\\\n').replace('\\t', '\\\\t'))\n",
    "            if typ == 'int':\n",
    "                value = str(value)\n",
    "            else:\n",
    "                if typ.endswith('char'):\n",
    "                    lValue = len(value)\n",
    "                    curlen = fieldSizes['word'][name]\n",
    "                    if lValue > curlen: fieldSizes['word'][name] = lValue\n",
    "                value = \"'{}'\".format(sEsc(value))\n",
    "            row.append(value)\n",
    "        tables['word'].append('({})'.format(','.join(row)))\n",
    "\n",
    "for n in N():\n",
    "    if F.otype.v(n) == 'book':\n",
    "        utils.caption(0, '\\t{}'.format(F.book.v(n)))\n",
    "    elif F.otype.v(n) == 'verse':\n",
    "        doVerseInfo(n)\n",
    "\n",
    "utils.caption(0, 'Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK    word           word_heb            : max size =      20 of    32\n",
      "OK    word           word_ktv            : max size =      27 of    32\n",
      "OK    word           word_vlex           : max size =       2 of    32\n",
      "OK    word           word_clex           : max size =      15 of    32\n",
      "OK    word           word_tran           : max size =      30 of    32\n",
      "OK    word           word_phono          : max size =      23 of    32\n",
      "OK    word           word_phono_sep      : max size =       2 of     8\n",
      "OK    word           word_lex            : max size =      15 of    32\n",
      "OK    word           word_glex           : max size =      24 of    32\n",
      "OK    word           word_gloss          : max size =      25 of    32\n",
      "OK    word           word_lang           : max size =       3 of     8\n",
      "OK    word           word_pos            : max size =       4 of     8\n",
      "OK    word           word_pdp            : max size =       4 of     8\n",
      "OK    word           word_subpos         : max size =       4 of     8\n",
      "OK    word           word_nmtp           : max size =      14 of    32\n",
      "OK    word           word_tense          : max size =       4 of     8\n",
      "OK    word           word_stem           : max size =       4 of     8\n",
      "OK    word           word_gender         : max size =       7 of     8\n",
      "OK    word           word_gnumber        : max size =       7 of     8\n",
      "OK    word           word_person         : max size =       7 of     8\n",
      "OK    word           word_state          : max size =       2 of     8\n",
      "OK    word           word_nme            : max size =       6 of     8\n",
      "OK    word           word_pfm            : max size =       6 of     8\n",
      "OK    word           word_prs            : max size =       6 of     8\n",
      "OK    word           word_uvf            : max size =       6 of     8\n",
      "OK    word           word_vbe            : max size =       3 of     8\n",
      "OK    word           word_vbs            : max size =       6 of     8\n",
      "OK    word           subphrase_border    : max size =       8 of    16\n",
      "OK    word           subphrase_number    : max size =      31 of    32\n",
      "OK    word           subphrase_rela      : max size =       3 of     8\n",
      "OK    word           phrase_border       : max size =       5 of     8\n",
      "OK    word           phrase_atom_rela    : max size =       4 of     8\n",
      "OK    word           phrase_function     : max size =       4 of     8\n",
      "OK    word           phrase_rela         : max size =       4 of     8\n",
      "OK    word           phrase_typ          : max size =       4 of     8\n",
      "OK    word           phrase_det          : max size =       3 of     8\n",
      "OK    word           clause_border       : max size =       5 of     8\n",
      "OK    word           clause_atom_pargr   : max size =      39 of    64\n",
      "OK    word           clause_rela         : max size =       4 of     8\n",
      "OK    word           clause_typ          : max size =       4 of     8\n",
      "OK    word           clause_txt          : max size =       7 of     8\n",
      "OK    word           sentence_border     : max size =       5 of     8\n"
     ]
    }
   ],
   "source": [
    "# check whether the field sizes are not exceeded\n",
    "\n",
    "tb = 'word'\n",
    "for f in wordFields:\n",
    "    (fl, typ, limit) = ('{}_{}'.format(f[2], f[1]), f[3], f[4])\n",
    "    if typ != 'varchar': continue\n",
    "    actual = fieldSizes[tb][fl]\n",
    "    exceeded = actual > limit\n",
    "    outp = sys.stderr if exceeded else sys.stdout\n",
    "    outp.write('{:<5} {:<15}{:<20}: max size = {:>7} of {:>5}\\n'.format(\n",
    "        'ERROR' if exceeded else 'OK',\n",
    "        tb, fl, actual, limit,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................\n",
      ".      4m 21s Generating SQL ...                                                             .\n",
      "..............................................................................................\n",
      "|      4m 21s \ttable book\n",
      "|      4m 21s \ttable chapter\n",
      "|      4m 21s \ttable verse\n",
      "|      4m 21s \ttable clause_atom\n",
      "|      4m 22s \ttable lexicon\n",
      "|      4m 22s \ttable word\n",
      "|      4m 22s \ttable word_verse\n",
      "|      4m 22s Done\n"
     ]
    }
   ],
   "source": [
    "limitRow = 2000\n",
    "\n",
    "tablesHead = collections.OrderedDict((\n",
    "    ('book', 'insert into book (id, first_m, last_m, name) values \\n'),\n",
    "    ('chapter', 'insert into chapter (id, first_m, last_m, book_id, chapter_num) values \\n'),\n",
    "    ('verse', 'insert into verse (id, first_m, last_m, chapter_id, verse_num, text, xml) values \\n'),\n",
    "    ('clause_atom', 'insert into clause_atom (id, first_m, last_m, ca_num, book_id, text) values \\n'),\n",
    "    ('lexicon', 'insert into lexicon ({}) values \\n'.format(', '.join(f[0] for f in lexFields))),\n",
    "    ('word', 'insert into word ({}) values \\n'.format(', '.join('{}_{}'.format(f[2], f[1]) for f in wordFields))),\n",
    "    ('word_verse', 'insert into word_verse (anchor, verse_id, lexicon_id) values \\n'),\n",
    "))\n",
    "\n",
    "sqf = open(mysqlFile, 'w')\n",
    "sqf.write(textCreateSql)\n",
    "\n",
    "utils.caption(4, 'Generating SQL ...')\n",
    "for table in tablesHead:\n",
    "    utils.caption(0, '\\ttable {}'.format(table))\n",
    "    start = tablesHead[table]\n",
    "    rows = tables[table]\n",
    "    r = 0\n",
    "    while r < len(rows):\n",
    "        sqf.write(start)\n",
    "        s = min(r + limitRow, len(rows))\n",
    "        sqf.write(' {}'.format(rows[r]))\n",
    "        if r + 1 < len(rows):\n",
    "            for t in rows[r + 1:s]: sqf.write('\\n,{}'.format(t))\n",
    "        sqf.write(';\\n')\n",
    "        r = s\n",
    "\n",
    "sqf.close()\n",
    "utils.caption(0, 'Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deliver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.gzip(mysqlFile, mysqlZFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
