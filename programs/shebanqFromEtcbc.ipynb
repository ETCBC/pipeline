{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" src=\"tf-small.png\"/>\n",
    "\n",
    "# SHEBANQ from ETCBC\n",
    "\n",
    "This notebook assembles the data from the ETCBC that is needed\n",
    "to feed the website [SHEBANQ](https://shebanq.ancient-data.org).\n",
    "\n",
    "All data is delivered through github repositories.\n",
    "Before the pipeline starts, these repos must be pulled.\n",
    "\n",
    "This notebook will call a series of other notebooks, some of them\n",
    "residing in other github repos.\n",
    "Before these notebooks can be run, they must be converted to Python\n",
    "programs. Then the will be called as such, with parameters injected as local variables.\n",
    "One of these parameters will be `SCRIPT=True`, with the understanding\n",
    "that a notebook can adapt its actions to the fact that it is part of the pipeline.\n",
    "These notebooks can also be run interactively, and then you can add extra actions which are not relevant to the pipeline conversion, such as testing, experimenting, visualizing.\n",
    "Take care that you wrap non-essential things in contexts where\n",
    "`SCRIPT=False`.\n",
    "\n",
    "This notebook itself can also be run in script mode.\n",
    "\n",
    "## Pipeline\n",
    "\n",
    "### Core data\n",
    "\n",
    "The core data is delivered by the ETCBC as `bhsa.mql.bz2` in \n",
    "the Github repo [bhsa](https://github.com/ETCBC/bhsa) in directory `source`.\n",
    "\n",
    "This data will be converted by `tfFromMQL` in the `programs` directory.\n",
    "\n",
    "The result of this action will be an updated TF resource in its \n",
    "`tf/core` directory.\n",
    "\n",
    "### Statistics\n",
    "\n",
    "The notebook `addStats` in the same *bhsa* repo will add statistical\n",
    "features to the core dataset: `freq_occ freq_lex rank_occ rank_lex`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os,sys,collections\n",
    "from pipeline import runPipeline\n",
    "from tf.fabric import Fabric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CORE_NAME = 'bhsa'\n",
    "CORE_MODULE = 'core'\n",
    "\n",
    "if 'SCRIPT' not in locals(): \n",
    "    SCRIPT = False\n",
    "    DEFAULT_CORE_NAME = CORE_NAME\n",
    "    DEFAULT_VERSION = 'c'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = dict(\n",
    "    defaults = dict(\n",
    "        CORE_NAME=CORE_NAME,\n",
    "        VERSION=DEFAULT_VERSION,\n",
    "        CORE_MODULE=CORE_MODULE,\n",
    "    ),\n",
    "    versions={\n",
    "        'c': dict(),\n",
    "        'd': dict(),\n",
    "        '2017': dict(),\n",
    "    },\n",
    "    repoOrderX='''\n",
    "        bhsa\n",
    "        phono\n",
    "        parallels\n",
    "        valence\n",
    "    ''',\n",
    "    repoOrder = '''\n",
    "        bhsa\n",
    "        phono\n",
    "    ''',\n",
    "    repoConfig = dict(\n",
    "        bhsa=(\n",
    "            dict(\n",
    "                task='tfFromMQL',\n",
    "            ),\n",
    "            dict(\n",
    "                task='addStats',\n",
    "            ),\n",
    "        ),\n",
    "        phono=(\n",
    "            dict(\n",
    "                task='phono',\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to run pipeline for version d\n",
      "START tfFromMQL (CORE_MODULE=core, CORE_NAME=bhsa, VERSION=d)\n",
      "Dataset in place\n",
      "SUCCESS tfFromMQL\n",
      "START addStats (CORE_MODULE=core, CORE_NAME=bhsa, VERSION=d)\n",
      "SUCCESS addStats\n",
      "START phono (CORE_MODULE=core, CORE_NAME=bhsa, VERSION=d)\n",
      "\tDestination /Users/dirk/github/etcbc/phono/tf/d/phono/.tf/phono.tfx does not exist\n",
      "This is Text-Fabric 2.3.12\n",
      "Api reference : https://github.com/ETCBC/text-fabric/wiki/Api\n",
      "Tutorial      : https://github.com/ETCBC/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Data sources  : https://github.com/ETCBC/text-fabric-data\n",
      "Data docs     : https://etcbc.github.io/text-fabric-data\n",
      "Shebanq docs  : https://shebanq.ancient-data.org/text\n",
      "Slack team    : https://shebanq.slack.com/signup\n",
      "Questions? Ask shebanq@ancient-data.org for an invite to Slack\n",
      "108 features found and 0 ignored\n",
      "  0.00s loading features ...\n",
      "   |     0.12s B g_cons               from /Users/dirk/github/etcbc/bhsa/tf/d/core\n",
      "   |     0.18s B g_cons_utf8          from /Users/dirk/github/etcbc/bhsa/tf/d/core\n",
      "   |     0.14s B g_word               from /Users/dirk/github/etcbc/bhsa/tf/d/core\n",
      "   |     0.20s B g_word_utf8          from /Users/dirk/github/etcbc/bhsa/tf/d/core\n",
      "   |     0.12s B lex0                 from /Users/dirk/github/etcbc/bhsa/tf/d/core\n",
      "   |     0.18s B lex_utf8             from /Users/dirk/github/etcbc/bhsa/tf/d/core\n",
      "   |     0.00s B qere                 from /Users/dirk/github/etcbc/bhsa/tf/d/core\n",
      "   |     0.00s B qere_trailer         from /Users/dirk/github/etcbc/bhsa/tf/d/core\n",
      "   |     0.08s B trailer              from /Users/dirk/github/etcbc/bhsa/tf/d/core\n",
      "   |     0.12s B lex                  from /Users/dirk/github/etcbc/bhsa/tf/d/core\n",
      "   |     0.12s B sp                   from /Users/dirk/github/etcbc/bhsa/tf/d/core\n",
      "   |     0.12s B vs                   from /Users/dirk/github/etcbc/bhsa/tf/d/core\n",
      "   |     0.12s B vt                   from /Users/dirk/github/etcbc/bhsa/tf/d/core\n",
      "   |     0.10s B gn                   from /Users/dirk/github/etcbc/bhsa/tf/d/core\n",
      "   |     0.12s B nu                   from /Users/dirk/github/etcbc/bhsa/tf/d/core\n",
      "   |     0.12s B ps                   from /Users/dirk/github/etcbc/bhsa/tf/d/core\n",
      "   |     0.10s B st                   from /Users/dirk/github/etcbc/bhsa/tf/d/core\n",
      "   |     0.12s B uvf                  from /Users/dirk/github/etcbc/bhsa/tf/d/core\n",
      "   |     0.12s B prs                  from /Users/dirk/github/etcbc/bhsa/tf/d/core\n",
      "   |     0.08s B g_prs                from /Users/dirk/github/etcbc/bhsa/tf/d/core\n",
      "   |     0.12s B pfm                  from /Users/dirk/github/etcbc/bhsa/tf/d/core\n",
      "   |     0.12s B vbs                  from /Users/dirk/github/etcbc/bhsa/tf/d/core\n",
      "   |     0.11s B vbe                  from /Users/dirk/github/etcbc/bhsa/tf/d/core\n",
      "   |     0.12s B language             from /Users/dirk/github/etcbc/bhsa/tf/d/core\n",
      "   |     0.00s Feature overview: 102 for nodes; 5 for edges; 1 configs; 7 computed\n",
      "  6.02s All features loaded/computed - for details use loadLog()\n",
      "  6.02s Looking for non-verb qamets\n",
      "  8.83s 4060 lexemes and 13458 unique occurrences\n",
      "  8.83s Filtering lexemes with varied occurrences\n",
      "  9.40s 161 interesting lexemes with 1705 unique occurrences\n",
      "  9.40s Guessing between gadol and qatan\n",
      "JM/: Override for syllable 1: ā becomes o\n",
      "BJT/: Override for syllable 1: o becomes ā\n",
      "JWMM: Override for syllable 2:  becomes ā\n",
      "JHWNTN/: Override for syllable 2:  becomes ā\n",
      "JRB<M/: No override needed for syllable 1 which is ā\n",
      "  9.44s 107 lexemes with modified occurrences (224)\n",
      "  9.44s 0 patterns with conflicts\n",
      "  9.44s Generating data in two ways ... \n",
      "    11s  1000 verses 13315 62 0 21\n",
      "    13s  2000 verses 27406 123 2 79\n",
      "    15s  3000 verses 40962 174 5 125\n",
      "    17s  4000 verses 54142 242 8 143\n",
      "    19s  5000 verses 67150 307 13 171\n",
      "    22s  6000 verses 82446 393 15 196\n",
      "    24s  7000 verses 97548 456 17 254\n",
      "    26s  8000 verses 113745 528 18 287\n",
      "    28s  9000 verses 129599 572 20 327\n",
      "    30s 10000 verses 146214 623 20 438\n",
      "    32s 11000 verses 159806 748 20 487\n",
      "    34s 12000 verses 174189 890 24 524\n",
      "    37s 13000 verses 190552 1017 28 576\n",
      "    39s 14000 verses 205101 1167 32 622\n",
      "    41s 15000 verses 218607 1289 33 728\n",
      "    42s 16000 verses 227941 1335 39 777\n",
      "    43s 17000 verses 235631 1378 48 827\n",
      "    45s 18000 verses 243254 1395 51 866\n",
      "    46s 19000 verses 250705 1428 59 906\n",
      "    47s 20000 verses 260114 1469 60 960\n",
      "    49s 21000 verses 275079 1532 63 979\n",
      "    50s 22000 verses 286437 1589 65 1007\n",
      "    53s 23000 verses 301295 1644 66 1075\n",
      "    53s 23213 verses done 304793 1649 66 1081\n",
      "  270184 accents\n",
      "    9015 cleanup\n",
      "   45235 dagesh_forte\n",
      "   21511 dagesh_forte_lene\n",
      "   59612 dagesh_lene\n",
      "   16321 default_accent\n",
      "     968 fixit\n",
      "    2658 furtive_patah\n",
      "   28195 last_ml\n",
      "    2201 mappiq_heh\n",
      "   93898 mobile_schwa1\n",
      "    2255 mobile_schwa2\n",
      "     179 mobile_schwa3\n",
      "    7702 mobile_schwa4\n",
      "   25498 punct\n",
      "   25498 punctuation\n",
      "      66 qamets_prs_suppress_qatan\n",
      "    5257 qamets_qatan1\n",
      "     243 qamets_qatan2\n",
      "    1791 qamets_qatan3\n",
      "      28 qamets_qatan4a\n",
      "     256 qamets_qatan4b\n",
      "     209 qamets_qatan5\n",
      "    1081 qamets_qatan_corrections\n",
      "    1649 qamets_verb_suppress_qatan\n",
      "      12 rafe\n",
      "   21098 silent_aleph\n",
      "  304793 total\n",
      "  304789 trim\n",
      "    53s 304793 items in phono\n",
      "    53s Reading word\n",
      "    53s 23213 lines\n",
      "OK: phono text and word info are CONSISTENT\n",
      "This is Text-Fabric 2.3.12\n",
      "Api reference : https://github.com/ETCBC/text-fabric/wiki/Api\n",
      "Tutorial      : https://github.com/ETCBC/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Data sources  : https://github.com/ETCBC/text-fabric-data\n",
      "Data docs     : https://etcbc.github.io/text-fabric-data\n",
      "Shebanq docs  : https://shebanq.ancient-data.org/text\n",
      "Slack team    : https://shebanq.slack.com/signup\n",
      "Questions? Ask shebanq@ancient-data.org for an invite to Slack\n",
      "0 features found and 0 ignored\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s Grid feature \"otype\" not found in\n",
      "\n",
      "  0.00s Grid feature \"oslots\" not found in\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Grid feature \"otext\" not found. Working without Text-API\n",
      "\n",
      "  0.00s Exporting 2 node and 0 edge and 1 config features to /Users/dirk/github/etcbc/phono/_temp/d/phono:\n",
      "   |     0.78s T phono                to /Users/dirk/github/etcbc/phono/_temp/d/phono\n",
      "   |     0.63s T phono_trailer        to /Users/dirk/github/etcbc/phono/_temp/d/phono\n",
      "   |     0.00s M otext@phono          to /Users/dirk/github/etcbc/phono/_temp/d/phono\n",
      "  1.41s Exported 2 node features and 0 edge features and 1 config features to /Users/dirk/github/etcbc/phono/_temp/d/phono\n",
      "      0.00s checkDiffs\n",
      "no features to add\n",
      "no features to delete\n",
      "2 features in common\n",
      "phono                     ... no changes\n",
      "phono_trailer             ... no changes\n",
      "Copy data set to /Users/dirk/github/etcbc/phono/tf/d/phono\n",
      "SUCCESS phono\n"
     ]
    }
   ],
   "source": [
    "good = runPipeline(pipeline, version='d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From here it is old stuff: do not run!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data model\n",
    "\n",
    "The data model of the browsing database as as follows:\n",
    "\n",
    "There are tables ``book``, ``chapter``, ``verse``, ``word_verse``, ``lexicon``, ``clause_atom``.\n",
    "\n",
    "The tables ``book``, ``chapter``, ``verse``, ``clause_atom`` contain fields ``first_m``, ``last_m``, \n",
    "denoting the first and last monad number of that book, chapter, verse, clause_atom.\n",
    "\n",
    "A ``book``-record contains an identifier and the name of the book.\n",
    "\n",
    "A ``chapter``-record contains an identifier, the number of the chapter, and a foreign key to the record in the ``book`` table to which the chapter belongs.\n",
    "\n",
    "A ``verse``-record contains an identifier, the number of the verse, and a foreign key to the record in the ``chapter`` table to which the verse belongs. More over, it contains the text of the whole verse in two formats:\n",
    "\n",
    "In field ``text``: the plain unicode text string of the complete verse.\n",
    "\n",
    "In field ``xml``: a sequence of ``<w>`` elements, one for each word in the verse, containing the plain unicode text string of that word as element content.\n",
    "The monad number of that word is stored in an attribute value. \n",
    "The monad number is a globally unique sequence number of a word occurrence in the Hebrew Bible, going from 1 to precisely 426,555.\n",
    "There is also a lexical identifier stored in an attribute value.\n",
    "The lexical identifier points to the lexical entry that corresponds with the word.\n",
    "\n",
    "    <w m=\"2\" l=\"3\">רֵאשִׁ֖ית </w>\n",
    "\n",
    "As you see, the material between a word and the next word is appended to the first word. So, when you concatenate words, whitespace or other separators are needed.\n",
    "\n",
    "A ``word_verse``-record links a word to a verse. \n",
    "The monad number is in field ``anchor``, which is an integer, \n",
    "and the verse is specified in the field ``verse_id`` as foreign key.\n",
    "The field ``lexicon_id`` is a foreign key into the ``lexicon`` table.\n",
    "\n",
    "There is also a ``word`` table, meant to store all the information to generate a rich representation of the hebrew text,\n",
    "its syntactic structure, and some linguistic properties.\n",
    "See that notebook for a description and an example of the rich hebrew text representation.\n",
    "\n",
    "The rich data is added per word, but the data has a dependency on the verses the words are contained in.\n",
    "In general, information about sentences, clauses and phrases will be displayed on the first words of those objects,\n",
    "but if the object started in a previous verse, this information is repeated on the first word of that object in the\n",
    "current verse.\n",
    "This insures that the display of a verse is always self-contained.\n",
    "\n",
    "The ``word`` table has no field ``id``, its primary key is the field called ``word_number``. \n",
    "This fields contains the same monad number as is used in the field ``anchor`` of the table ``word_verse``.\n",
    "\n",
    "A ``clause_atom`` record contains an identifier, and the book to which it belongs, and its sequence number within \n",
    "that book.\n",
    "In SHEBANQ, manual annotations are linked to the clause atom, so we need this information to easily fetch comments to\n",
    "passages and to compose charts and csv files.\n",
    "\n",
    "## Lexicon\n",
    "\n",
    "A ``lexicon`` record contains the various lexical fields, such as identifiers, entry representations,\n",
    "additional lexical properties, and a gloss.\n",
    "\n",
    "We make sure that we translate lexical feature values into values used for the etcbc4.\n",
    "We need the following information per entry:\n",
    "\n",
    "* **id** a fresh id (see below), to be used in applications, unique over **entryid** and **lan**\n",
    "* **lan** the language of the entry, in ISO 639-3 abbreviation\n",
    "* **entryid** the string used as entry in the lexicon and as value of the ``lex`` feature in the text\n",
    "* **g_entryid** the Hebrew untransliteration of entryid, with the disambiguation marks unchanged, corresponds to the ``lex_utf8`` feature\n",
    "* **entry** the unpointed transliteration (= **entryid** without disambiguation marks)\n",
    "* **entry_heb** the unpointed hebrew representation, obtained by untransliterating **entry**\n",
    "* **g_entry** the pointed transliteration, without disambiguation marks, obtained from ``vc``\n",
    "* **g_entry_heb** the pointed hebrew representation, obtained by untransliterating **g_entry**\n",
    "* **root** the root, obtained from ``rt``\n",
    "* **pos** the part of speech, obtained from ``sp``\n",
    "* **nametype** the type of named entity, obtained from ``sm``\n",
    "* **subpos** subtype of part of speech, obtained from ``ls`` (aka *lexical set*)\n",
    "* **gloss** the gloss from ``gl``\n",
    "\n",
    "We construct the **id** from the ``lex`` feature as follows:\n",
    "\n",
    "* allocate a varchar(32)\n",
    "* the > is an alef, we translate it to A\n",
    "* the < is an ayin, we translate it to O\n",
    "* the / denotes a noun, we translate it to n\n",
    "* the [ denotes a verb, we translate it to v\n",
    "* the = is for disambiguation, we translate it to i\n",
    "* we prepend a language identifier, 1 for Hebrew, 2 for aramaic.\n",
    "\n",
    "This is sound, see the scheck in the extradata/lexicon notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Field transformation\n",
    "\n",
    "The lexical fields require a bit of attention.\n",
    "The specification in ``lex_fields`` below specifies the lexicon fields in the intended order.\n",
    "It contains instructions how to construct the field values from the lexical information obtained from the lexicon files.\n",
    "\n",
    "    (source, method, name, transformation table, data type, data size, data options, params)\n",
    "\n",
    "## source \n",
    "May contain one of the following:\n",
    "\n",
    "* the name of a lexical feature as shown in the lexicon files, such as ``sp``, ``vc``.\n",
    "* None. \n",
    "  In this case, **method** is a code that triggers special actions, such as getting an id or something that is available to the   program that fills the lexicon table\n",
    "* the name of an other field as shown in the **name** part of the specification. \n",
    "  In this case, **method** is a function, defined else where, that takes the value of that other field as argument. \n",
    "  The function is typically a transliteration, or a stripping action.\n",
    "\n",
    "## method\n",
    "May contain one of the following:\n",
    "\n",
    "* a code (string), indicating:\n",
    "    * ``lex``: take the value of a feature (indicated in **source**) for this entry from the lexicon file\n",
    "    * ``entry``: take the value of the entry itself as found in the lexicon file\n",
    "    * ``id``: take the id for this entry as generated by the program\n",
    "    * ``lan``: take the language of this entry\n",
    "* a function taking one argument\n",
    "    * *strip_id*: strip the non-lexeme characters at the end of the entry (the ``/ [ =`` characters)\n",
    "    * *to_heb*: transform the transliteration into real unicode Hebrew\n",
    "    * feature lookup functions such as ``F.lex.v``\n",
    "\n",
    "## name\n",
    "The name of the field in the to be constructed table ``lexicon`` in the database ``passage``.\n",
    "\n",
    "## data type\n",
    "The sql data type, such as ``int`` or ``varchar``, without the size and options.\n",
    "\n",
    "## data size\n",
    "The sql data size, which shows up between ``()`` after the data type\n",
    "\n",
    "## data options\n",
    "Any remaining type specification, such as `` character set utf8``.\n",
    "\n",
    "## params\n",
    "Params consists currently of 1 boolean, indicating whether the field is defined on all words of the object, or only on its first word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index of ketiv/qere\n",
    "\n",
    "We make a list of the ketiv-qere items.\n",
    "It will be used by the *heb* and the *ktv* functions.\n",
    "\n",
    "*heb()* provides the surface text of a word.\n",
    "When the qere is different from the ketiv, the vocalized qere is chosen.\n",
    "It is the value of ``g_word_utf8`` except when a qere is present, \n",
    "in which case it is ``g_qere_utf8``, preceded by a masora circle.\n",
    "This is the sign for the user to use data view to inspect the *ketiv*.\n",
    "\n",
    "*ktv()* provides the surface text of a word, in case the ketiv is different from the qere.\n",
    "It is the value of ``g_word_utf8`` precisely when a qere is present, \n",
    "otherwise it is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qeres = {}\n",
    "masora = '֯'\n",
    "msg('Building qere index')\n",
    "for w in F.g_qere_utf8.s():\n",
    "    qeres[w] = (masora+F.g_qere_utf8.v(w), F.qtrailer_utf8.v(w))\n",
    "msg('Found {} qeres'.format(len(qeres)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index of paragraphs\n",
    "\n",
    "We make a list of paragraph numbers of clause_atoms. It will be used by the *para* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paras = {}\n",
    "msg('Building para index')\n",
    "inf = infile('pargr_data.mql')\n",
    "first = True\n",
    "ca_index = {}\n",
    "for c in F.otype.s('clause_atom'):\n",
    "    ca_index[F.oid.v(c)] = c\n",
    "\n",
    "for line in inf:\n",
    "    if first:\n",
    "        first = False\n",
    "        continue\n",
    "    (oid, par) = line.rstrip('\\n').split('\\t')\n",
    "    paras[ca_index[oid]] = par\n",
    "msg('Found para information for {} clause_atoms'.format(len(paras)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Field types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strip_id(entryid):\n",
    "    return entryid.rstrip('/[=')\n",
    "\n",
    "def to_heb(translit):\n",
    "    return Transcription.to_hebrew(Transcription.suffix_and_finales(translit)[0])\n",
    "\n",
    "def heb(n):\n",
    "    if n in qeres:\n",
    "        (trsep, wrdrep) = qeres[n]\n",
    "    else:\n",
    "        trsep = F.trailer_utf8.v(n)\n",
    "        wrdrep = F.g_word_utf8.v(n)\n",
    "    if trsep.endswith('ס') or trsep.endswith('פ'): trsep += ' '\n",
    "    return wrdrep + trsep\n",
    "\n",
    "def ktv(n):\n",
    "    if n in qeres:\n",
    "        trsep = F.trailer_utf8.v(n)\n",
    "        if trsep.endswith('ס') or trsep.endswith('פ'): trsep += ' '\n",
    "        return F.g_word_utf8.v(n) + trsep    \n",
    "    return ''\n",
    "\n",
    "def para(n): return paras.get(n, '')\n",
    "\n",
    "def lang(n):\n",
    "    return 'hbo' if F.language.v(n) == 'Hebrew' else 'arc'\n",
    "\n",
    "def df(f):\n",
    "    def g(n): \n",
    "        val = f(n)\n",
    "#        if val == None or val == \"None\" or val == \"none\" or val == \"NA\" or val == \"N/A\" or val == \"n/a\":\n",
    "        if val == None:\n",
    "            return '#null'\n",
    "        return val\n",
    "    return g\n",
    "\n",
    "lex_fields = (\n",
    "    (None, 'id', 'id', None, 'varchar', 32, ' primary key'),\n",
    "    (None, 'lan', 'lan', None, 'char', 4, ''),\n",
    "    (None, 'entry', 'entryid', None, 'varchar', 32, ''),\n",
    "    ('entryid', strip_id, 'entry', None, 'varchar', 32, ''),\n",
    "    ('entry', to_heb, 'entry_heb', None, 'varchar', 32, ' character set utf8'),\n",
    "    ('entryid', to_heb, 'entryid_heb', None, 'varchar', 32, ' character set utf8'),\n",
    "    ('vc', 'lex', 'g_entry', None, 'varchar', 32, ''),\n",
    "    ('g_entry', to_heb, 'g_entry_heb', None, 'varchar', 32, ' character set utf8'),\n",
    "    ('rt', 'lex', 'root', None, 'varchar', 32, ''),\n",
    "    ('sp', 'lex', 'pos', None, 'varchar', 8, ''),\n",
    "    ('sm', 'lex', 'nametype', None, 'varchar', 16, ''),\n",
    "    ('ls', 'lex', 'subpos', None, 'varchar', 8, ''),\n",
    "    ('gl', 'lex', 'gloss', None, 'varchar', 32, ' character set utf8'),\n",
    ")\n",
    "word_fields = (\n",
    "    (F.monads.v, 'number', 'word', 'int', 4, ' primary key', False),\n",
    "    (heb, 'heb', 'word', 'varchar', 32, '', False),\n",
    "    (ktv, 'ktv', 'word', 'varchar', 32, '', False),\n",
    "    (F.g_entry_heb.v, 'vlex', 'word', 'varchar', 32, '', False),\n",
    "    (F.lex_utf8.v, 'clex', 'word', 'varchar', 32, '', False),\n",
    "    (F.g_word.v, 'tran', 'word', 'varchar', 32, '', False),\n",
    "    (F.phono.v, 'phono', 'word', 'varchar', 32, '', False),\n",
    "    (F.phono_sep.v, 'phono_sep', 'word', 'varchar', 8, '', False),\n",
    "    (F.lex.v, 'lex', 'word', 'varchar', 32, '', False),\n",
    "    (F.g_lex.v, 'glex', 'word', 'varchar', 32, '', False),\n",
    "    (F.gloss.v, 'gloss', 'word', 'varchar', 32, '', False),\n",
    "    (lang, 'lang', 'word', 'varchar', 8, '', False),\n",
    "    (df(F.sp.v), 'pos', 'word', 'varchar', 8, '', False),\n",
    "    (df(F.pdp.v), 'pdp', 'word', 'varchar', 8, '', False),\n",
    "    (df(F.ls.v), 'subpos', 'word', 'varchar', 8, '', False),\n",
    "    (df(F.nametype.v), 'nmtp', 'word', 'varchar', 32, '', False),\n",
    "    (df(F.vt.v), 'tense', 'word', 'varchar', 8, '', False),\n",
    "    (df(F.vs.v), 'stem', 'word', 'varchar', 8, '', False),\n",
    "    (df(F.gn.v), 'gender', 'word', 'varchar', 8, '', False),\n",
    "    (df(F.nu.v), 'gnumber', 'word', 'varchar', 8, '', False),\n",
    "    (df(F.ps.v), 'person', 'word', 'varchar', 8, '', False),\n",
    "    (df(F.st.v), 'state', 'word', 'varchar', 8, '', False),\n",
    "    (df(F.nme.v), 'nme', 'word', 'varchar', 8, '', False),\n",
    "    (df(F.pfm.v), 'pfm', 'word', 'varchar', 8, '', False),\n",
    "    (df(F.prs.v), 'prs', 'word', 'varchar', 8, '', False),\n",
    "    (df(F.uvf.v), 'uvf', 'word', 'varchar', 8, '', False),\n",
    "    (df(F.vbe.v), 'vbe', 'word', 'varchar', 8, '', False),\n",
    "    (df(F.vbs.v), 'vbs', 'word', 'varchar', 8, '', False),\n",
    "    (F.freq_lex.v, 'freq_lex', 'word', 'int', 4, '', False),\n",
    "    (F.freq_occ.v, 'freq_occ', 'word', 'int', 4, '', False),\n",
    "    (F.rank_lex.v, 'rank_lex', 'word', 'int', 4, '', False),\n",
    "    (F.rank_occ.v, 'rank_occ', 'word', 'int', 4, '', False),\n",
    "    (None, 'border', 'subphrase', 'varchar', 16, '', False),\n",
    "    ('id', 'number', 'subphrase', 'varchar', 32, '', False),\n",
    "    (df(F.rela.v), 'rela', 'subphrase', 'varchar', 8, '', True),\n",
    "    (None, 'border', 'phrase', 'varchar', 8, '', False),\n",
    "    (F.number.v, 'number', 'phrase_atom', 'int', 4, '', False),\n",
    "    (df(F.rela.v), 'rela', 'phrase_atom', 'varchar', 8, '', True),\n",
    "    (F.number.v, 'number', 'phrase', 'int', 4, '', False),\n",
    "    (df(F.function.v), 'function', 'phrase', 'varchar', 8, '', True),\n",
    "    (df(F.rela.v), 'rela', 'phrase', 'varchar', 8, '', True),\n",
    "    (df(F.typ.v), 'typ', 'phrase', 'varchar', 8, '', True),\n",
    "    (df(F.det.v), 'det', 'phrase', 'varchar', 8, '', True),\n",
    "    (None, 'border', 'clause', 'varchar', 8, '', False),\n",
    "    (F.number.v, 'number', 'clause_atom', 'int', 4, '', False),\n",
    "    (df(F.code.v), 'code', 'clause_atom', 'varchar', 8, '', True),\n",
    "    (df(F.tab.v), 'tab', 'clause_atom', 'int', 4, '', False),\n",
    "    (para, 'pargr', 'clause_atom', 'varchar', 64, '', True),\n",
    "    (F.number.v, 'number', 'clause', 'int', 4, '', False),\n",
    "    (df(F.rela.v), 'rela', 'clause', 'varchar', 8, '', True),\n",
    "    (df(F.typ.v), 'typ', 'clause', 'varchar', 8, '', True),\n",
    "    (df(F.txt.v), 'txt', 'clause', 'varchar', 8, '', False),\n",
    "    (None, 'border', 'sentence', 'varchar', 8, '', False),\n",
    "    (F.number.v, 'number', 'sentence_atom', 'int', 4, '', False),\n",
    "    (F.number.v, 'number', 'sentence', 'int', 4, '', False),\n",
    ")\n",
    "first_only = dict(('{}_{}'.format(f[2], f[1]), f[6]) for f in word_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity\n",
    "The texts and xml representations of verses are stored in ``varchar`` fields.\n",
    "We have to make sure that the values fit within the declared sizes of these fields.\n",
    "The code measures the maximum lengths of these fields, and it turns out that the text is maximally 434 chars and the xml 2186 chars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "field_limits = {\n",
    "    'book': {\n",
    "        'name': 32,\n",
    "    },\n",
    "    'verse': {\n",
    "        'text': 1024,\n",
    "        'xml': 4096,\n",
    "    },\n",
    "    'clause_atom': {\n",
    "        'text': 512,\n",
    "    },\n",
    "    'lexicon': {},\n",
    "}\n",
    "for f in lex_fields:\n",
    "    if f[4].endswith('char'):\n",
    "        field_limits['lexicon'][f[2]] = f[5]\n",
    "\n",
    "config = {\n",
    "    'db': 'shebanq_passage'+version,\n",
    "}\n",
    "for tb in field_limits:\n",
    "    for fl in field_limits[tb]: config['{}_{}'.format(tb, fl)] = field_limits[tb][fl]\n",
    "\n",
    "text_create_sql = '''\n",
    "drop database if exists {db};\n",
    "\n",
    "create database {db} character set utf8;\n",
    "\n",
    "use {db};\n",
    "\n",
    "create table book(\n",
    "    id      int(4) primary key,\n",
    "    first_m int(4),\n",
    "    last_m int(4),\n",
    "    name varchar({book_name}),\n",
    "    index(name)\n",
    ");\n",
    "\n",
    "create table chapter(\n",
    "    id int(4) primary key,\n",
    "    first_m int(4),\n",
    "    last_m int(4),\n",
    "    book_id int(4),\n",
    "    chapter_num int(4),\n",
    "    foreign key (book_id) references book(id),\n",
    "    index(chapter_num)\n",
    ");\n",
    "\n",
    "create table verse(\n",
    "    id int(4) primary key,\n",
    "    first_m int(4),\n",
    "    last_m int(4),\n",
    "    chapter_id int(4),\n",
    "    verse_num int(4),\n",
    "    text varchar({verse_text}) character set utf8,\n",
    "    xml varchar({verse_xml}) character set utf8,\n",
    "    foreign key (chapter_id) references chapter(id)\n",
    ");\n",
    "\n",
    "create table clause_atom(\n",
    "    id int(4) primary key,\n",
    "    first_m int(4),\n",
    "    last_m int(4),\n",
    "    ca_num int(4),    \n",
    "    book_id int(4),\n",
    "    text varchar({clause_atom_text}) character set utf8,\n",
    "    foreign key (book_id) references book(id),\n",
    "    index(ca_num)\n",
    ");\n",
    "\n",
    "create table word(\n",
    "    {{wordfields}}\n",
    ");\n",
    "\n",
    "create table lexicon(\n",
    "    {{lexfields}}    \n",
    ") collate utf8_bin;\n",
    "\n",
    "create table word_verse(\n",
    "    anchor int(4) unique,\n",
    "    verse_id int(4),\n",
    "    lexicon_id varchar(32),\n",
    "    foreign key (anchor) references word(word_number),\n",
    "    foreign key (verse_id) references verse(id),\n",
    "    foreign key (lexicon_id) references lexicon(id)\n",
    ") collate utf8_bin;\n",
    "\n",
    "'''.format(**config).format(\n",
    "        lexfields = ',\\n    '.join('{} {}({}){}'.format(\n",
    "            f[2], f[4], f[5], f[6],\n",
    "        ) for f in lex_fields),\n",
    "        wordfields = ', \\n    '.join('{}_{} {}({}){}'.format(\n",
    "            f[2], f[1], f[3], f[4], f[5],\n",
    "    ) for f in word_fields),\n",
    ")\n",
    "print(text_create_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexicon file reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = {'hbo', 'arc'}\n",
    "lex_base = dict((lan, '{}/{}/{}.{}{}'.format(API['data_dir'], 'lexicon', lan, source, version)) for lan in langs)\n",
    "lang_map = {\n",
    "    'Hebrew': 'hbo',\n",
    "    'Aramaic': 'arc',\n",
    "}\n",
    "\n",
    "def read_lex(lan):\n",
    "    lex_infile = open(lex_base[lan], encoding='utf-8')\n",
    "\n",
    "    lex_items = {}\n",
    "    ln = 0\n",
    "    e = 0\n",
    "    for line in lex_infile:\n",
    "        ln += 1\n",
    "        line = line.split('#')[0]\n",
    "        line = line.rstrip()\n",
    "        if line == '': continue\n",
    "        (entry, featurestr) = line.split(sep=None, maxsplit=1)\n",
    "        entry = entry.strip('\"')\n",
    "        if entry in lex_items:\n",
    "            sys.stderr.write('duplicate lexical entry {} in line {}.\\n'.format(entry, ln))\n",
    "            e += 1\n",
    "            continue\n",
    "        if featurestr.startswith(':') and featurestr.endswith(':'):\n",
    "            featurestr = featurestr.strip(':')\n",
    "        featurestr = featurestr.replace('\\\\:', chr(254))\n",
    "        featurelst = featurestr.split(':')\n",
    "        features = {}\n",
    "        for feature in featurelst:\n",
    "            comps = feature.split('=', maxsplit=1)\n",
    "            if len(comps) == 1:\n",
    "                if feature.strip().isnumeric():\n",
    "                    comps = ('_n', feature.strip())\n",
    "                else:\n",
    "                    sys.stderr.write('feature without value for lexical entry {} in line {}: {}\\n'.format(entry, ln, feature))\n",
    "                    e += 1\n",
    "                    continue\n",
    "            (key, value) = comps\n",
    "            value = value.replace(chr(254), ':')\n",
    "            if key in features:\n",
    "                sys.stderr.write('duplicate feature for lexical entry {} in line {}: {}={}\\n'.format(entry, ln, key, value))\n",
    "                e += 1\n",
    "                continue\n",
    "            features[key] = value\n",
    "        if 'sp' in features and features['sp'] == 'verb':\n",
    "            if 'gl' in features:\n",
    "                gloss = features['gl']\n",
    "                if gloss.startswith('to '):\n",
    "                    features['gl'] = gloss[3:]\n",
    "        lex_items[entry] = features\n",
    "        \n",
    "    lex_infile.close()\n",
    "    msgstr = \"Lexicon {}: there w\".format(lan) + ('ere {} errors'.format(e) if e != 1 else 'as 1 error') + '\\n'\n",
    "    sys.stderr.write(msgstr)\n",
    "    return lex_items\n",
    "\n",
    "lex_entries = dict((lan, read_lex(lan)) for lan in sorted(langs))\n",
    "for lan in sorted(lex_entries):\n",
    "    print('Lexicon {} has {:>5} entries'.format(lan, len(lex_entries[lan])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexicon result\n",
    "The result is also stored in a tab separated file, which can be downloaded from my\n",
    "[SURFdrive](https://surfdrive.surf.nl/files/public.php?service=files&t=f910f1e088d1dfc9fc526e408ab07c45)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table filling\n",
    "\n",
    "We compose all the records for all the tables.\n",
    "\n",
    "We also generate a file that can act as the basis of an extra annotation file with lexical information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg(\"Fill the tables ... \")\n",
    "cur_id = {\n",
    "    'book': -1,\n",
    "    'chapter': - 1,\n",
    "    'verse': -1,\n",
    "    'clause_atom': -1\n",
    "}\n",
    "\n",
    "def s_esc(sql): return sql.replace(\"'\", \"''\").replace('\\\\','\\\\\\\\').replace('\\n','\\\\n')\n",
    "\n",
    "cur_verse_node = None\n",
    "cur_verse_info = []\n",
    "cur_verse_first_m = None\n",
    "cur_verse_last_m = None\n",
    "cur_lex_values = {}\n",
    "\n",
    "lex_index = {}\n",
    "lex_not_found = collections.defaultdict(lambda: collections.Counter())\n",
    "tables = collections.defaultdict(lambda: [])\n",
    "field_sizes = collections.defaultdict(lambda: collections.defaultdict(lambda: 0))\n",
    "\n",
    "Fotypev = F.otype.v\n",
    "Fmonadsv = F.monads.v\n",
    "Fmin = F.minmonad.v\n",
    "Fmax = F.maxmonad.v\n",
    "Ftextv = F.g_word_utf8.v\n",
    "Foccv = F.g_cons.v\n",
    "Flexv = F.lex.v\n",
    "Flanguagev = F.language.v\n",
    "Ftrailerv = F.trailer_utf8.v\n",
    "Fnumberv = F.number.v\n",
    "\n",
    "dqf = outfile('etcbc4-lexicon.tsv')\n",
    "dqf.write('{}\\n'.format('\\t'.join(x[2] for x in lex_fields)))\n",
    "\n",
    "def compute_fields(lan, entry, lexfeats):\n",
    "    cur_lex_values.clear()\n",
    "    return tuple(compute_field(lan, entry, lexfeats, f) for f in lex_fields)\n",
    "\n",
    "def compute_field(lan, entry, lexfeats, f):\n",
    "    (source, method, name, transform, datatype, datasize, dataoption) = f\n",
    "    val = None\n",
    "    if method == 'lan': val = lan\n",
    "    elif method == 'entry': val = entry\n",
    "    elif method == 'id':\n",
    "        val = '{}{}'.format(\n",
    "            '1' if lan == 'hbo' else '2',\n",
    "            entry.\n",
    "                replace('>','A').\n",
    "                replace('<','O').\n",
    "                replace('[','v').\n",
    "                replace('/','n').\n",
    "                replace('=','i'),\n",
    "        )\n",
    "        lex_index[(lan, entry)] = val\n",
    "    elif method =='lex':\n",
    "        val = s_esc(lexfeats.get(source, ''))\n",
    "        if transform != None and val in transform: val = transform[val]\n",
    "    else: val = method(cur_lex_values[source])\n",
    "    cur_lex_values[name] = val\n",
    "    if name in field_limits['lexicon']:\n",
    "        field_sizes['lexicon'][name] = max(len(val), field_sizes['lexicon'][name])\n",
    "    return val\n",
    "\n",
    "for lan in sorted(lex_entries):\n",
    "    for entry in sorted(lex_entries[lan]):\n",
    "        format_str = '({})'.format(','.join('{}' if f[4] == 'int' else \"'{}'\" for f in lex_fields))\n",
    "        entry_info = compute_fields(lan, entry, lex_entries[lan][entry])\n",
    "        dqf.write('{}\\n'.format('\\t'.join(str(x) for x in entry_info)))\n",
    "        tables['lexicon'].append(format_str.format(\n",
    "            *entry_info\n",
    "        ))\n",
    "dqf.close()\n",
    "\n",
    "def do_verse(node):\n",
    "    global cur_verse_node, cur_verse_info, max_len_text, max_len_xml\n",
    "    if cur_verse_node != None:\n",
    "        this_text = ''.join('{}{}'.format(x[0], x[1]) for x in cur_verse_info)\n",
    "        this_xml = ''.join(\n",
    "            '''<w m=\"{}\" t=\"{}\" l=\"{}\">{}</w>'''.format(\n",
    "                x[2], x[1].replace('\\n', '&#xa;'), x[4], x[0]\n",
    "            ) for x in cur_verse_info)\n",
    "        field_sizes['verse']['text'] = max((len(this_text), field_sizes['verse']['text']))\n",
    "        field_sizes['verse']['xml'] = max((len(this_xml), field_sizes['verse']['xml']))\n",
    "        tables['verse'].append(\"({},{},{},{},{},'{}','{}')\".format(\n",
    "            cur_id['verse'], \n",
    "            cur_verse_first_m, \n",
    "            cur_verse_last_m, \n",
    "            cur_id['chapter'], F.verse.v(cur_verse_node), s_esc(this_text), s_esc(this_xml),\n",
    "        ))\n",
    "        for x in cur_verse_info:\n",
    "            tables['word_verse'].append(\"({}, {}, '{}')\".format(\n",
    "                x[2], x[3], x[4]\n",
    "            ))\n",
    "        cur_verse_info = []\n",
    "    cur_verse_node = node    \n",
    "\n",
    "for node in NN():\n",
    "    otype = Fotypev(node)\n",
    "    if otype == 'word':\n",
    "        if node in qeres:\n",
    "            (text, trailer) = qeres[node]\n",
    "        else:\n",
    "            text = Ftextv(node)\n",
    "            trailer = Ftrailerv(node)\n",
    "        if trailer.endswith('ס') or trailer.endswith('פ'): trailer += ' '\n",
    "        lex = Flexv(node)\n",
    "        lang = Flanguagev(node)\n",
    "        lid = lex_index.get((lang_map[lang], lex), None)\n",
    "        if lid == None:\n",
    "            lex_not_found[(lang_map[lang], lex)][Foccv(node)] += 1\n",
    "        cur_verse_info.append((\n",
    "            text,\n",
    "            trailer,\n",
    "            Fmonadsv(node), \n",
    "            cur_id['verse'],\n",
    "            lid,\n",
    "        ))\n",
    "    elif otype == 'verse':\n",
    "        do_verse(node)\n",
    "        cur_id['verse'] += 1\n",
    "        cur_verse_first_m = Fmin(node)\n",
    "        cur_verse_last_m = Fmax(node)\n",
    "    elif otype == 'chapter':\n",
    "        do_verse(None)\n",
    "        cur_id['chapter'] += 1\n",
    "        tables['chapter'].append(\"({},{},{},{},{})\".format(\n",
    "            cur_id['chapter'], Fmin(node), Fmax(node), cur_id['book'], F.chapter.v(node),\n",
    "        ))\n",
    "    elif otype == 'book':\n",
    "        do_verse(None)\n",
    "        cur_id['book'] += 1\n",
    "        name = F.book.v(node)\n",
    "        field_sizes['book']['name'] = max((len(name), field_sizes['book']['name']))\n",
    "        tables['book'].append(\"({},{},{},'{}')\".format(\n",
    "            cur_id['book'], Fmin(node), Fmax(node), s_esc(name),\n",
    "        ))\n",
    "    elif otype == 'clause_atom':\n",
    "        cur_id['clause_atom'] += 1\n",
    "        ca_num = Fnumberv(node)\n",
    "        wordtexts = []\n",
    "        for w in L.d('word', node):\n",
    "            trsep = Ftrailerv(w)\n",
    "            if trsep.endswith('ס') or trsep.endswith('פ'): trsep += ' '\n",
    "            wordtexts.append(F.g_word_utf8.v(w) +trsep)\n",
    "        text = ''.join(wordtexts)\n",
    "        field_sizes['clause_atom']['text'] = max((len(text), field_sizes['clause_atom']['text']))\n",
    "        tables['clause_atom'].append(\"({},{},{},{},{},'{}')\".format(\n",
    "            cur_id['clause_atom'], Fmin(node), Fmax(node), ca_num, cur_id['book'], s_esc(text),\n",
    "        ))\n",
    "do_verse(None)\n",
    "\n",
    "for tb in sorted(field_limits):\n",
    "    for fl in sorted(field_limits[tb]):\n",
    "        limit = field_limits[tb][fl]\n",
    "        actual = field_sizes[tb][fl]\n",
    "        exceeded = actual > limit\n",
    "        outp = sys.stderr if exceeded else sys.stdout\n",
    "        outp.write('{:<5} {:<15}{:<15}: max size = {:>7} of {:>5}\\n'.format(\n",
    "            'ERROR' if exceeded else 'OK',\n",
    "            tb, fl, actual, limit,\n",
    "        ))\n",
    "\n",
    "msg(\"Done\")\n",
    "if len(lex_not_found):\n",
    "    sys.stderr.write('Text lexemes not found in lexicon: {}x\\n'.format(len(lex_not_found)))\n",
    "    for l in sorted(lex_not_found):\n",
    "        sys.stderr.write('{} {}\\n'.format(*l))\n",
    "        for (o, n) in sorted(lex_not_found[l].items(), key=lambda x: (-x[1], x[0])):\n",
    "            sys.stderr.write('\\t{}: {}x\\n'.format(o, n))\n",
    "else:\n",
    "    print('All lexemes have been found in the lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n'.join(tables['lexicon'][0:10]))\n",
    "print('\\n'.join(tables['clause_atom'][0:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra word data\n",
    "\n",
    "Now we fetch the data needed for representing rich hebrew text.\n",
    "\n",
    "## Passage index\n",
    "When we have found our objects, we want to indicate where they occur in the bible. In order to specify the passage of a node, we have to now in what verse a node occurs. In the next code cell we create a mapping from nodes of type sentence, clause, etc to nodes of type verse. From a verse node we can read off the passage information.\n",
    "\n",
    "Conversely, we also construct an index from verses to nodes: given a verse, we make a list of all nodes belonging to that verse, in the canonical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_types = {\n",
    "    'sentence', 'sentence_atom', \n",
    "    'clause', 'clause_atom', \n",
    "    'phrase', 'phrase_atom', \n",
    "    'subphrase',\n",
    "}\n",
    "\n",
    "def get_set(monads):\n",
    "    monad_set = set()\n",
    "    for rn in monads.split(','):\n",
    "        bnds = rn.split('-', 1)\n",
    "        if len(bnds) == 1:\n",
    "            monad_set.add(int(bnds[0]))\n",
    "        else: \n",
    "            monad_set |= set(range(int(bnds[0]), int(bnds[1]) + 1))\n",
    "    return frozenset(monad_set)\n",
    "\n",
    "def ranges(monadset):\n",
    "    result = []\n",
    "    cur_start = None\n",
    "    cur_end = None\n",
    "    for i in sorted(monadset):\n",
    "        if cur_start == None:\n",
    "            cur_start = i\n",
    "            cur_end = i\n",
    "        else:\n",
    "            if i == cur_end + 1:\n",
    "                cur_end += 1\n",
    "            else:\n",
    "                result.append((cur_start, cur_end))\n",
    "                cur_start = i\n",
    "                cur_end = i\n",
    "    if cur_start != None:\n",
    "        result.append((cur_start, cur_end))\n",
    "    return result\n",
    "\n",
    "def get_objects(vn):\n",
    "    objects = set()\n",
    "    for wn in L.d('word', vn):\n",
    "        objects.add(wn)\n",
    "        for tt in target_types:\n",
    "            on = L.u(tt, wn)\n",
    "            if on != None: objects.add(on)\n",
    "    return objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill the word info table with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg(\"Generating word info data ...\")\n",
    "wordf = outfile('word_data.tsv')\n",
    "#wordrf = outfile('word_r_data.tsv')\n",
    "plainf = outfile('verse_plain.txt')\n",
    "wordf.write('{}\\n'.format('\\t'.join('{}_{}'.format(f[2], f[1]) for f in word_fields)))\n",
    "#wordrf.write('{}\\t{}\\t{}\\t{}\\n'.format('book', 'chapter', 'verse', '\\t'.join('{}_{}'.format(f[2], f[1]) for f in word_fields)))\n",
    "tables['word'] = []\n",
    "\n",
    "if 'word' in field_sizes: del field_sizes['word']\n",
    "\n",
    "def do_verse_info(verse):\n",
    "    vlabel = '{} {}:{}'.format(F.book.v(verse), F.chapter.v(verse), F.verse.v(verse))\n",
    "    wordf.write('!{}\\n'.format(vlabel))\n",
    "#    monads = verse_monads[verse]\n",
    "    monads = {int(F.monads.v(w)) for w in L.d('word', verse)}\n",
    "    \n",
    "    (verse_startm, verse_endm) = (min(monads), max(monads))\n",
    "#    objects = verse_node[verse]\n",
    "    objects = get_objects(verse)\n",
    "    words = [dict() for i in range(verse_startm, verse_endm + 1)]\n",
    "    for w in words:\n",
    "        for (otype, do_border) in (\n",
    "            ('sentence', True), \n",
    "            ('sentence_atom', False), \n",
    "            ('clause', True), \n",
    "            ('clause_atom', False), \n",
    "            ('phrase', True),\n",
    "            ('phrase_atom', False),\n",
    "            ('subphrase', True),\n",
    "            ('word', False),\n",
    "        ):\n",
    "            w['{}_{}'.format(otype, 'number')] = list()\n",
    "            if do_border:\n",
    "                w['{}_{}'.format(otype, 'border')] = set()\n",
    "    nwords = len(words)\n",
    "    subphrase_counter = 0\n",
    "    word_nodes = []\n",
    "    for n in objects:\n",
    "        otype = F.otype.v(n)\n",
    "        if otype == 'word': word_nodes.append(n)\n",
    "        number_prop = '{}_{}'.format(otype, 'number')\n",
    "        if otype != 'word' and not otype.endswith('_atom'):\n",
    "            border_prop = '{}_{}'.format(otype, 'border')\n",
    "        else:\n",
    "            border_prop = None\n",
    "\n",
    "        if otype == 'subphrase': subphrase_counter += 1\n",
    "        elif otype in {'phrase', 'clause', 'sentence'}: subphrase_counter = 0\n",
    "# Here was a bug: I put the subphrase_counter to 0 upon encountering anything else than a subphrase or a word.\n",
    "# I had overlooked the half_verse, which can cut through a phrase\n",
    "        this_info = {}\n",
    "        this_number = None\n",
    "        for f in word_fields:\n",
    "            (method, name, typ) = (f[0], '{}_{}'.format(f[2], f[1]), f[3])\n",
    "            if otype != f[2] or method == None: continue\n",
    "            if method == 'id':\n",
    "                value = subphrase_counter\n",
    "            else:\n",
    "                value = method(n)\n",
    "                if typ == 'int': value = int(value)\n",
    "            if name == number_prop:\n",
    "                this_number = value\n",
    "            else:\n",
    "                this_info[name] = value\n",
    "        if otype == 'word':\n",
    "            target = words[this_number - verse_startm]\n",
    "            target.update(this_info)\n",
    "            target[number_prop].append(this_number)            \n",
    "        else:\n",
    "            these_ranges = ranges(get_set(F.monads.v(n)))\n",
    "            nranges = len(these_ranges) - 1\n",
    "            for (e,r) in enumerate(these_ranges):\n",
    "                is_first = e == 0\n",
    "                is_last = e == nranges\n",
    "                right_border = 'rr' if is_first else 'r'\n",
    "                left_border = 'll' if is_last else 'l'\n",
    "                first_word = -1 if r[0] < verse_startm else nwords if r[0] > verse_endm else r[0] - verse_startm\n",
    "                last_word = -1 if r[1] < verse_startm else nwords if r[1] > verse_endm else r[1] - verse_startm\n",
    "                my_first_word = max(first_word, 0)\n",
    "                my_last_word = min(last_word, nwords - 1)\n",
    "                for i in range(my_first_word, my_last_word + 1):\n",
    "                    target = words[i]\n",
    "                    if not first_only[number_prop] or i == my_first_word:\n",
    "                        target[number_prop].append(this_number)\n",
    "                    for f in this_info:\n",
    "                        if not first_only[name] or i == my_first_word:\n",
    "                            words[i][f] = this_info[f]\n",
    "                    if otype == 'subphrase':\n",
    "                        if border_prop != None: words[i][border_prop].add('sy')\n",
    "                if 0 <= first_word < nwords:\n",
    "                    if border_prop != None: words[first_word][border_prop].add(right_border)\n",
    "                if 0 <= last_word < nwords:\n",
    "                    if border_prop != None: words[last_word][border_prop].add(left_border)\n",
    "    wordtext = []\n",
    "    for w in word_nodes:\n",
    "        trsep = Ftrailerv(w)\n",
    "        if trsep.endswith('ס') or trsep.endswith('פ'): trsep += ' '\n",
    "        wordtext.append(F.g_word_utf8.v(w) +trsep)\n",
    "    plainf.write(\"{}\\t{}\\n\".format(\n",
    "        vlabel, \n",
    "        ''.join(wt for wt in wordtext).replace('\\n', '\\\\n'),\n",
    "    ))\n",
    "    for w in words:\n",
    "        row = []\n",
    "        rrow = []\n",
    "        for f in word_fields:\n",
    "            typ = f[3]\n",
    "            name = '{}_{}'.format(f[2], f[1])\n",
    "            value = w.get(name, 'NULL' if typ == 'int' else '')\n",
    "            if f[1] == 'border':\n",
    "                value = ' '.join(value)\n",
    "            elif f[1] == 'number':\n",
    "                value = ' '.join(str(v) for v in value)\n",
    "            rrow.append(str(value).replace('\\n', '\\\\n').replace('\\t', '\\\\t'))\n",
    "            if typ == 'int':\n",
    "                value = str(value)\n",
    "            else:\n",
    "                if typ.endswith('char'):\n",
    "                    lvalue = len(value)\n",
    "                    curlen = field_sizes['word'][name]\n",
    "                    if lvalue > curlen: field_sizes['word'][name] = lvalue\n",
    "                value = \"'{}'\".format(s_esc(value))\n",
    "            row.append(value)\n",
    "        tables['word'].append('({})'.format(','.join(row)))\n",
    "        wordf.write(\"{}\\n\".format('\\t'.join(rrow)))\n",
    "        #wordrf.write(\"{}\\t{}\\t{}\\t{}\\n\".format(F.book.v(verse), F.chapter.v(verse), F.verse.v(verse),'\\t'.join(rrow)))\n",
    "\n",
    "for n in NN():\n",
    "    if F.otype.v(n) == 'book':\n",
    "        msg(\"\\t{}\".format(F.book.v(n)))\n",
    "    elif F.otype.v(n) == 'verse':\n",
    "        do_verse_info(n)\n",
    "\n",
    "wordf.close()\n",
    "#wordrf.close()\n",
    "plainf.close()\n",
    "msg(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether the field sizes are not exceeded\n",
    "\n",
    "tb = 'word'\n",
    "for f in word_fields:\n",
    "    (fl, typ, limit) = ('{}_{}'.format(f[2], f[1]), f[3], f[4])\n",
    "    if typ != 'varchar': continue\n",
    "    actual = field_sizes[tb][fl]\n",
    "    exceeded = actual > limit\n",
    "    outp = sys.stderr if exceeded else sys.stdout\n",
    "    outp.write('{:<5} {:<15}{:<20}: max size = {:>7} of {:>5}\\n'.format(\n",
    "        'ERROR' if exceeded else 'OK',\n",
    "        tb, fl, actual, limit,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_row = 2000\n",
    "\n",
    "tables_head = collections.OrderedDict((\n",
    "    ('book', 'insert into book (id, first_m, last_m, name) values \\n'),\n",
    "    ('chapter', 'insert into chapter (id, first_m, last_m, book_id, chapter_num) values \\n'),\n",
    "    ('verse', 'insert into verse (id, first_m, last_m, chapter_id, verse_num, text, xml) values \\n'),\n",
    "    ('clause_atom', 'insert into clause_atom (id, first_m, last_m, ca_num, book_id, text) values \\n'),\n",
    "    ('lexicon', 'insert into lexicon ({}) values \\n'.format(', '.join(f[2] for f in lex_fields))),\n",
    "    ('word', 'insert into word ({}) values \\n'.format(', '.join('{}_{}'.format(f[2], f[1]) for f in word_fields))),\n",
    "    ('word_verse', 'insert into word_verse (anchor, verse_id, lexicon_id) values \\n'),\n",
    "))\n",
    "\n",
    "sqf = outfile('shebanq_passage{}.sql'.format(version))\n",
    "sqf.write(text_create_sql)\n",
    "\n",
    "msg('Generating SQL ...')\n",
    "for table in tables_head:\n",
    "    msg('\\ttable {}'.format(table))\n",
    "    start = tables_head[table]\n",
    "    rows = tables[table]\n",
    "    r = 0\n",
    "    while r < len(rows):\n",
    "        sqf.write(start)\n",
    "        s = min(r + limit_row, len(rows))\n",
    "        sqf.write(' {}'.format(rows[r]))\n",
    "        if r + 1 < len(rows):\n",
    "            for t in rows[r + 1:s]: sqf.write('\\n,{}'.format(t))\n",
    "        sqf.write(';\\n')\n",
    "        r = s\n",
    "        \n",
    "sqf.close()\n",
    "msg('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
